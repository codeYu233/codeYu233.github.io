<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Recommendation</title>
      <link href="/2023/09/24/Recommendation/"/>
      <url>/2023/09/24/Recommendation/</url>
      
        <content type="html"><![CDATA[<h1 id="Recommendation"><a href="#Recommendation" class="headerlink" title="Recommendation"></a><em><strong>Recommendation</strong></em></h1><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><p>In this part of the exercise, you will implement the collaborative filtering learning algorithm and apply it to a dataset of movie ratings.2 This dataset consists of ratings on a scale of 1 to 5. The dataset has <em>n**u</em> &#x3D; 943 users, and n*<em>m</em> &#x3D; 1682 movies.</p><h2 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h2><p>推荐算法是当今各大IT、科技、视频公司的研究重点，因为这些公司往往有精准投放广告和精准商品推荐的需求。本次采用的是较为基础的协同过滤算法，算法简单，在梯度下降的基础上实现。只需将电影特征参数，用户特征参数作为未知数，构造损失函数，利用梯度下降求得最优值即可。该算法需要有一定的数据集供训练，也就是从许多用户的电影打分评价情况来进行拟合，推测电影特征、用户特征，并进行预测。该算法较为基础，一般需要大量数据才能保证预测的准确度。目前多数公司还是采用神经网络等手段进行推荐算法的实现。</p><h2 id="数据读取处理"><a href="#数据读取处理" class="headerlink" title="数据读取处理"></a>数据读取处理</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import scipy.io as sio</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">mat=sio.loadmat(&#x27;Recommendation_movies.mat&#x27;)</span><br><span class="line">mat.keys()</span><br><span class="line">dict_keys([&#x27;__header__&#x27;, &#x27;__version__&#x27;, &#x27;__globals__&#x27;, &#x27;Y&#x27;, &#x27;R&#x27;])</span><br><span class="line"></span><br><span class="line">Y,R=mat[&#x27;Y&#x27;],mat[&#x27;R&#x27;]</span><br><span class="line">Y.shape,R.shape</span><br><span class="line">((1682, 943), (1682, 943))</span><br><span class="line"></span><br><span class="line">param_mat=sio.loadmat(&#x27;Recommendation_movieParams.mat&#x27;)</span><br><span class="line">param_mat.keys()</span><br><span class="line">dict_keys([&#x27;__header__&#x27;, &#x27;__version__&#x27;, &#x27;__globals__&#x27;, &#x27;X&#x27;, &#x27;Theta&#x27;, &#x27;num_users&#x27;, &#x27;num_movies&#x27;, &#x27;num_features&#x27;])</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X,Theta,nu,nm,nf=param_mat[&#x27;X&#x27;],param_mat[&#x27;Theta&#x27;],param_mat[&#x27;num_users&#x27;],param_mat[&#x27;num_movies&#x27;],param_mat[&#x27;num_features&#x27;]</span><br><span class="line"></span><br><span class="line">X.shape,Theta.shape,nu,nm,nf</span><br><span class="line">((1682, 10),</span><br><span class="line"> (943, 10),</span><br><span class="line"> array([[943]], dtype=uint16),</span><br><span class="line"> array([[1682]], dtype=uint16),</span><br><span class="line"> array([[10]], dtype=uint8))</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">nu=int(nu)</span><br><span class="line">nm=int(nm)</span><br><span class="line">nf=int(nf)</span><br><span class="line">nu,nm,nf</span><br><span class="line">(943, 1682, 10)</span><br></pre></td></tr></table></figure><h2 id="序列化和还原"><a href="#序列化和还原" class="headerlink" title="序列化和还原"></a>序列化和还原</h2><p>因为minimize函数必须传入一维数据，所以我们要先将特征参数矩阵序列化，完成训练后再还原</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">serialize</span>(<span class="params">X,Theta</span>):</span><br><span class="line">    <span class="keyword">return</span> np.append(X.flatten(),Theta.flatten())</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">deserialize</span>(<span class="params">params,nm,nu,nf</span>):</span><br><span class="line">    X=params[:nm*nf].reshape(nm,nf)</span><br><span class="line">    Theta=params[nm*nf:].reshape(nu,nf)</span><br><span class="line">    <span class="keyword">return</span> X,Theta</span><br></pre></td></tr></table></figure><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def costFunction(params,Y,R,nm,nu,nf,lamda):</span><br><span class="line">    X,Theta=deserialize(params,nm,nu,nf)</span><br><span class="line">    error=0.5*np.square((X@Theta.T-Y)*R).sum()</span><br><span class="line">    reg1=0.5*lamda*np.square(X).sum()</span><br><span class="line">    reg2=0.5*lamda*np.square(Theta).sum()</span><br><span class="line">    return error+reg1+reg2</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">users=4</span><br><span class="line">movies=5</span><br><span class="line">features=3</span><br><span class="line">X_sub=X[:movies,:features]</span><br><span class="line">Theta_sub=Theta[:users,:features]</span><br><span class="line">Y_sub=Y[:movies,:users]</span><br><span class="line">R_sub=R[:movies,:users]</span><br><span class="line">cost1=costFunction(serialize(X_sub,Theta_sub),Y_sub,R_sub,movies,users,features,lamda=0)</span><br><span class="line">cost1</span><br><span class="line">22.224603725685675</span><br></pre></td></tr></table></figure><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">costGradient</span>(<span class="params">params,Y,R,nm,nu,nf,lamda</span>):</span><br><span class="line">    X,Theta=deserialize(params,nm,nu,nf)</span><br><span class="line">    X_grad=((X@Theta.T-Y)*R)@Theta+lamda*X</span><br><span class="line">    Theta_grad=((X@Theta.T-Y)*R).T@X+lamda*Theta</span><br><span class="line">    <span class="keyword">return</span> serialize(X_grad,Theta_grad)</span><br></pre></td></tr></table></figure><h2 id="创建一个测试用用户"><a href="#创建一个测试用用户" class="headerlink" title="创建一个测试用用户"></a>创建一个测试用用户</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">my_ratings=np.zeros((nm,1))</span><br><span class="line">my_ratings[27]=5</span><br><span class="line">my_ratings[28]=5</span><br><span class="line">my_ratings[49]=5</span><br><span class="line">my_ratings[134]=5</span><br><span class="line">my_ratings[140]=5</span><br><span class="line">my_ratings[175]=5</span><br><span class="line">my_ratings[200]=2</span><br><span class="line">my_ratings[203]=4</span><br><span class="line">my_ratings[226]=5</span><br><span class="line">my_ratings[227]=5</span><br><span class="line">my_ratings[228]=5</span><br><span class="line">my_ratings[229]=5</span><br><span class="line">my_ratings[230]=5</span><br><span class="line">my_ratings[342]=5</span><br><span class="line">my_ratings[402]=5</span><br><span class="line">my_ratings[422]=5</span><br><span class="line">my_ratings[448]=5</span><br><span class="line">my_ratings[449]=5</span><br><span class="line"></span><br><span class="line">Y=np.c_[Y,my_ratings]</span><br><span class="line">R=np.c_[R,my_ratings!=0]</span><br><span class="line">Y.shape</span><br><span class="line">(1682, 944)</span><br><span class="line"></span><br><span class="line">nm,nu=Y.shape</span><br></pre></td></tr></table></figure><h2 id="将打分归一化"><a href="#将打分归一化" class="headerlink" title="将打分归一化"></a>将打分归一化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">normalizeRatings</span>(<span class="params">Y,R</span>):</span><br><span class="line">    Y_mean=(Y.<span class="built_in">sum</span>(axis=<span class="number">1</span>)/R.<span class="built_in">sum</span>(axis=<span class="number">1</span>)).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    Y_norm=(Y-Y_mean)*R</span><br><span class="line">    <span class="keyword">return</span> Y_norm,Y_mean</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Y_norm,Y_mean=normalizeRatings(Y,R)</span><br></pre></td></tr></table></figure><h2 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X=np.random.random((nm,nf))</span><br><span class="line">Theta=np.random.random((nu,nf))</span><br><span class="line">params=serialize(X,Theta)</span><br><span class="line">lamda=5</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize</span><br><span class="line">res=minimize(fun=costFunction,x0=params,args=(Y_norm,R,nm,nu,nf,lamda),method=<span class="string">&#x27;TNC&#x27;</span>,jac=costGradient,options=&#123;<span class="string">&#x27;maxiter&#x27;</span>:<span class="number">100</span>&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">params_fit=res.x</span><br><span class="line">fit_X,fit_Theta=deserialize(params_fit,nm,nu,nf)</span><br><span class="line">Y_pred=fit_X@fit_Theta.T</span><br></pre></td></tr></table></figure><h2 id="对测试用用户进行预测并打印结果"><a href="#对测试用用户进行预测并打印结果" class="headerlink" title="对测试用用户进行预测并打印结果"></a>对测试用用户进行预测并打印结果</h2><p>我特意令测试用用户针对部分电影打高分，比如星球大战、星际迷航、太空漫步、Alien等科幻、冒险电影。在打分数据较多的情况下，可以看见结果还是比较靠谱，排在前面的有独立日、星战系列、终结者、夺宝奇兵等。因为这些电影带有一定的爱情、浪漫情节、元素，结果同样也推荐了相关的泰坦尼克号等电影。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">y_pred=Y_pred[:,-1]+Y_mean.flatten()</span><br><span class="line">index=np.argsort(-y_pred)</span><br><span class="line">index[:10]</span><br><span class="line">array([ 120,   49,  180,  171,  173,   95,  312,  194, 1466,  209],</span><br><span class="line">      dtype=int64)</span><br><span class="line"></span><br><span class="line">movies=[]</span><br><span class="line">with open(&#x27;movie_ids.txt&#x27;,&#x27;r&#x27;,encoding=&#x27;latin 1&#x27;)as f:</span><br><span class="line">    for line in f:</span><br><span class="line">        tokens=line.strip().split(&#x27; &#x27;)</span><br><span class="line">        movies.append(&#x27; &#x27;.join(tokens[1:]))</span><br><span class="line"></span><br><span class="line">len(movies)</span><br><span class="line">1682</span><br><span class="line"></span><br><span class="line">for i in range(10):</span><br><span class="line">    print(index[i],movies[index[i]],y_pred[index[i]])</span><br><span class="line"></span><br><span class="line">120 Independence Day (ID4) (1996) 5.449120842440642</span><br><span class="line">49 Star Wars (1977) 5.352125183811118</span><br><span class="line">180 Return of the Jedi (1983) 5.319959550159248</span><br><span class="line">171 Empire Strikes Back, The (1980) 5.182303379024724</span><br><span class="line">173 Raiders of the Lost Ark (1981) 5.170746823498372</span><br><span class="line">95 Terminator 2: Judgment Day (1991) 5.109846973143337</span><br><span class="line">312 Titanic (1997) 5.042844020713496</span><br><span class="line">194 Terminator, The (1984) 5.026389017143506</span><br><span class="line">1466 Saint of Fort Washington, The (1993) 5.0090202813542275</span><br><span class="line">209 Indiana Jones and the Last Crusade (1989) 5.008614086836323</span><br></pre></td></tr></table></figure><h2 id="Site"><a href="#Site" class="headerlink" title="Site"></a>Site</h2><p>代码(Jupyter)和所用数据：<a href="https://github.com/codeYu233/Study/tree/main/Recommendation">https://github.com/codeYu233/Study/tree/main/Recommendation</a></p><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>该题与数据集均来源于Coursera上斯坦福大学的吴恩达老师机器学习的习题作业，学习交流用，如有不妥，立马删除</p>]]></content>
      
      
      <categories>
          
          <category> Study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Program </tag>
            
            <tag> ML </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Anomaly Detection</title>
      <link href="/2023/09/23/Anomaly%20Detection/"/>
      <url>/2023/09/23/Anomaly%20Detection/</url>
      
        <content type="html"><![CDATA[<h1 id="Anomaly-Detection"><a href="#Anomaly-Detection" class="headerlink" title="Anomaly Detection"></a><em><strong>Anomaly Detection</strong></em></h1><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><p>In this exercise, you will implement an anomaly detection algorithm to detect anomalous behavior in server computers. The features measure the through put (mb&#x2F;s) and latency (ms) of response of each server. While your servers were operating, you collected <em>m</em> &#x3D; 307 examples of how they were behaving,</p><p>and thus have an unlabeled dataset <em>{*<em>x</em> (1)*, . . . , x</em>(<em>m</em>)<em>}</em>. You suspect that the vast majority of these examples are “normal” (non-anomalous) examples of the servers operating normally, but there might also be some examples of servers acting anomalously within this dataset.</p><h2 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h2><p>在实际生产中，我们借助于计算机通常会不断得到很多数据反馈，（比如飞机运行时的发动机温度转速、风扇转速、颠簸度等等）正确判断反馈的数据是否正常是整个系统安全平稳运行的基础，因此数据的异常检测极为重要。常见的异常检测其实跟“机器学习”的关联性并不是很大，体现机器学习的地方可能就是通过计算机选取很多个epsilon来找到契合模型的最佳的epsilon。普通的异常检测的本质是对于高斯函数的运用，我们需要对数据进行处理，使其能形成一个近似高斯函数，然后选取最佳的epsilon，在epsilon限定范围以内的点记为异常点。具体epsilon的选取方式为：得分＝2×精确率×召回率&#x2F;（精确率＋召回率）。得分越大epsilon选取越好。</p><h2 id="数据读取处理"><a href="#数据读取处理" class="headerlink" title="数据读取处理"></a>数据读取处理</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import scipy.io as sio</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">mat=sio.loadmat(&#x27;Anomaly Detection.mat&#x27;)</span><br><span class="line">mat.keys()</span><br><span class="line">dict_keys([&#x27;__header__&#x27;, &#x27;__version__&#x27;, &#x27;__globals__&#x27;, &#x27;X&#x27;, &#x27;Xval&#x27;, &#x27;yval&#x27;])</span><br><span class="line"></span><br><span class="line">X=mat[&#x27;X&#x27;]</span><br><span class="line">Xval,yval=mat[&#x27;Xval&#x27;],mat[&#x27;yval&#x27;]</span><br><span class="line">X.shape,Xval.shape,yval.shape</span><br><span class="line">((307, 2), (307, 2), (307, 1))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],<span class="string">&#x27;bx&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>​<br><img src="/2023/09/23/Anomaly%20Detection/output_3_0.png" alt="png"><br>​    </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">estimateGaussian</span>(<span class="params">X,isCovariance</span>):</span><br><span class="line">    means=np.mean(X,axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">if</span> isCovariance:</span><br><span class="line">        sigma2=(X-means).T@(X-means)/<span class="built_in">len</span>(X)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        sigma2=np.var(X,axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> means,sigma2</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">means,sigma2=estimateGaussian(X,isCovariance=True)</span><br><span class="line">sigma2</span><br><span class="line">array([[ 1.83263141, -0.22712233],</span><br><span class="line">       [-0.22712233,  1.70974533]])</span><br><span class="line"></span><br><span class="line">means,sigma2=estimateGaussian(X,isCovariance=False)</span><br><span class="line">sigma2</span><br><span class="line">array([1.83263141, 1.70974533])</span><br></pre></td></tr></table></figure><h2 id="高斯函数的构建"><a href="#高斯函数的构建" class="headerlink" title="高斯函数的构建"></a>高斯函数的构建</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gaussian</span>(<span class="params">X,means,sigma2</span>):</span><br><span class="line">    <span class="keyword">if</span> np.ndim(sigma2)==<span class="number">1</span>:</span><br><span class="line">        sigma2=np.diag(sigma2)</span><br><span class="line">    X=X-means</span><br><span class="line">    n=X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    first=np.power(<span class="number">2</span>*np.pi,-n/<span class="number">2</span>)*(np.linalg.det(sigma2)**(-<span class="number">0.5</span>))</span><br><span class="line">    second=np.diag(X@np.linalg.inv(sigma2)@X.T)</span><br><span class="line">    p=first*np.exp(-<span class="number">0.5</span>*second)</span><br><span class="line">    p=p.reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> p</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plotGaussian</span>(<span class="params">X,means,sigma2</span>):</span><br><span class="line">    x=np.arange(<span class="number">0</span>,<span class="number">30</span>,<span class="number">0.5</span>)</span><br><span class="line">    y=np.arange(<span class="number">0</span>,<span class="number">30</span>,<span class="number">0.5</span>)</span><br><span class="line">    xx,yy=np.meshgrid(x,y)</span><br><span class="line">    z=gaussian(np.c_[xx.ravel(),yy.ravel()],means,sigma2)</span><br><span class="line">    zz=z.reshape(xx.shape)</span><br><span class="line">    plt.plot(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],<span class="string">&#x27;bx&#x27;</span>)</span><br><span class="line">    contour_levels=[<span class="number">10</span>**h <span class="keyword">for</span> h <span class="keyword">in</span> <span class="built_in">range</span>(-<span class="number">20</span>,<span class="number">0</span>,<span class="number">3</span>)]</span><br><span class="line">    plt.contour(xx,yy,zz,contour_levels)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">means,sigma2=estimateGaussian(X,isCovariance=<span class="literal">False</span>)</span><br><span class="line">plotGaussian(X,means,sigma2)</span><br></pre></td></tr></table></figure><p>​<br><img src="/2023/09/23/Anomaly%20Detection/output_9_0.png" alt="png"><br>​    </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">means,sigma2=estimateGaussian(X,isCovariance=<span class="literal">True</span>)</span><br><span class="line">plotGaussian(X,means,sigma2)</span><br></pre></td></tr></table></figure><p>​<br><img src="/2023/09/23/Anomaly%20Detection/output_10_0.png" alt="png"><br>​    </p><h2 id="选取最佳的Epsilon"><a href="#选取最佳的Epsilon" class="headerlink" title="选取最佳的Epsilon"></a>选取最佳的Epsilon</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">selectThreshold</span>(<span class="params">yval,p</span>):</span><br><span class="line">    bestEpsilon=<span class="number">0</span></span><br><span class="line">    bestF1=<span class="number">0</span></span><br><span class="line">    epsilons=np.linspace(<span class="built_in">min</span>(p),<span class="built_in">max</span>(p),<span class="number">1000</span>)</span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> epsilons:</span><br><span class="line">        p_=p&lt;e</span><br><span class="line">        tp=np.<span class="built_in">sum</span>((yval==<span class="number">1</span>)&amp;(p_==<span class="number">1</span>))</span><br><span class="line">        fp=np.<span class="built_in">sum</span>((yval==<span class="number">0</span>)&amp;(p_==<span class="number">1</span>))</span><br><span class="line">        fn=np.<span class="built_in">sum</span>((yval==<span class="number">1</span>)&amp;(p_==<span class="number">0</span>))</span><br><span class="line">        prec=tp/(tp+fp)<span class="keyword">if</span>(tp+fp) <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        rec=tp/(tp+fn)<span class="keyword">if</span>(tp+fn) <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        F1_e=<span class="number">2</span>*prec*rec/(prec+rec)<span class="keyword">if</span>(prec+rec) <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> F1_e&gt;bestF1:</span><br><span class="line">            bestF1=F1_e</span><br><span class="line">            bestEpsilon=e</span><br><span class="line">    <span class="keyword">return</span> bestEpsilon,bestF1</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">means,sigma2=estimateGaussian(X,isCovariance=True)</span><br><span class="line">pval=gaussian(Xval,means,sigma2)</span><br><span class="line">bestEpsilon,bestF1=selectThreshold(yval,pval)</span><br><span class="line"></span><br><span class="line">bestEpsilon,bestF1</span><br><span class="line">(array([9.07484457e-05]), 0.8750000000000001)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">p=gaussian(X,means,sigma2)</span><br><span class="line">anoms=np.array([X[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">0</span>])<span class="keyword">if</span> p[i]&lt;bestEpsilon])</span><br><span class="line">plotGaussian(X,means,sigma2)</span><br><span class="line">plt.scatter(anoms[:,<span class="number">0</span>],anoms[:,<span class="number">1</span>],c=<span class="string">&#x27;r&#x27;</span>,marker=<span class="string">&#x27;o&#x27;</span>)</span><br></pre></td></tr></table></figure><p>​<br><img src="/2023/09/23/Anomaly%20Detection/output_14_1.png" alt="png"><br>​    </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">means,sigma2=estimateGaussian(X,isCovariance=False)</span><br><span class="line">pval=gaussian(Xval,means,sigma2)</span><br><span class="line">bestEpsilon,bestF1=selectThreshold(yval,pval)</span><br><span class="line"></span><br><span class="line">bestEpsilon,bestF1</span><br><span class="line">(array([8.99985263e-05]), 0.8750000000000001)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">p=gaussian(X,means,sigma2)</span><br><span class="line">anoms=np.array([X[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">0</span>])<span class="keyword">if</span> p[i]&lt;bestEpsilon])</span><br><span class="line">plotGaussian(X,means,sigma2)</span><br><span class="line">plt.scatter(anoms[:,<span class="number">0</span>],anoms[:,<span class="number">1</span>],c=<span class="string">&#x27;r&#x27;</span>,marker=<span class="string">&#x27;o&#x27;</span>)</span><br></pre></td></tr></table></figure><p>​<br><img src="/2023/09/23/Anomaly%20Detection/output_17_1.png" alt="png"><br>​    </p><h2 id="高维情况"><a href="#高维情况" class="headerlink" title="高维情况"></a>高维情况</h2><p>可以看出是否协方差对于异常点的估计有影响</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mat=sio.loadmat(&#x27;Anomaly Detection_2.mat&#x27;)</span><br><span class="line">X2=mat[&#x27;X&#x27;]</span><br><span class="line">Xval2,yval2=mat[&#x27;Xval&#x27;],mat[&#x27;yval&#x27;]</span><br><span class="line">X2.shape</span><br><span class="line"></span><br><span class="line">(1000, 11)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">means,sigma2=estimateGaussian(X2,isCovariance=False)</span><br><span class="line">pval=gaussian(Xval2,means,sigma2)</span><br><span class="line">bestEpsilon,bestF1=selectThreshold(yval2,pval)</span><br><span class="line">p=gaussian(X2,means,sigma2)</span><br><span class="line">anoms=[X2[i] for i in range(len(X2)) if p[i]&lt;bestEpsilon]</span><br><span class="line">len(anoms)</span><br><span class="line">117</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">means,sigma2=estimateGaussian(X2,isCovariance=True)</span><br><span class="line">pval=gaussian(Xval2,means,sigma2)</span><br><span class="line">bestEpsilon,bestF1=selectThreshold(yval2,pval)</span><br><span class="line">p=gaussian(X2,means,sigma2)</span><br><span class="line">anoms=[X2[i] for i in range(len(X2)) if p[i]&lt;bestEpsilon]</span><br><span class="line">len(anoms)</span><br><span class="line">122</span><br></pre></td></tr></table></figure><h2 id="Site"><a href="#Site" class="headerlink" title="Site"></a>Site</h2><p>代码(Jupyter)和所用数据：<a href="https://github.com/codeYu233/Study/tree/main/Anomaly%20Detection">https://github.com/codeYu233/Study/tree/main/Anomaly%20Detection</a></p><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>该题与数据集均来源于Coursera上斯坦福大学的吴恩达老师机器学习的习题作业，学习交流用，如有不妥，立马删除</p>]]></content>
      
      
      <categories>
          
          <category> Study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Program </tag>
            
            <tag> ML </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PCA</title>
      <link href="/2023/09/19/PCA/"/>
      <url>/2023/09/19/PCA/</url>
      
        <content type="html"><![CDATA[<h1 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a><em><strong>PCA</strong></em></h1><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><p>In this part of the exercise, you will run PCA on face images to see how it can be used in practice for dimension reduction. </p><h2 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h2><p>PCA是利用矩阵向量的数学知识对目标数据进行降维的操作。比如以汽车为单位进行机器学习，车长，车高往往有较大变化，但车宽变化较小。我们就可以将原来多个轴映射到一条新轴上，实现对数据向量个数的削减。PCA在许多年前能广泛运用于机器学习算法的加速，那时计算机能容纳的数据量有限，运算速度也相对较慢，利用PCA对数据降维可以很好地实现机器学习。但随着时代的发展，现在已经不需要用PCA就能实现对数据的加工处理，加上PCA反而会导致数据一定程度上的损失。现阶段，PCA虽然已不能在学习算法中举足轻重，但我们依然可以利用PCA将数据可视化。因为绘制高维图像是不可能的，利用PCA将数据维度降低到立体空间或者平面，是一种很好的可视化操作。这里通过对图片数据进行PCA降维，来直观展现降维对于数据整体的影响。</p><h2 id="数据读取处理"><a href="#数据读取处理" class="headerlink" title="数据读取处理"></a>数据读取处理</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import scipy.io as sio</span><br><span class="line">import matplotlib.pyplot as plt</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mat=sio.loadmat(&#x27;PCA.mat&#x27;)</span><br><span class="line">X=mat[&#x27;X&#x27;]</span><br><span class="line">X.shape</span><br><span class="line">(5000, 1024)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_100_image</span>(<span class="params">X</span>):</span><br><span class="line">    fig,axs=plt.subplots(ncols=<span class="number">10</span>,nrows=<span class="number">10</span>,figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):        </span><br><span class="line">            axs[c,r].imshow(X[<span class="number">10</span>*c+r].reshape(<span class="number">32</span>,<span class="number">32</span>).T,cmap=<span class="string">&#x27;Greys_r&#x27;</span>)</span><br><span class="line">            axs[c,r].set_xticks([])</span><br><span class="line">            axs[c,r].set_yticks([])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_100_image(X)</span><br></pre></td></tr></table></figure><p><img src="/2023/09/19/PCA/output_3_0.png" alt="png"></p><h2 id="数据降维操作"><a href="#数据降维操作" class="headerlink" title="数据降维操作"></a>数据降维操作</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">means=np.mean(X,axis=0)</span><br><span class="line">X_demean=X-means</span><br><span class="line">C=X_demean.T@X_demean</span><br><span class="line">U,S,V=np.linalg.svd(C)</span><br><span class="line"></span><br><span class="line">U1=U[:,:36]</span><br><span class="line">X_reduction=X_demean@U1</span><br><span class="line">X_reduction.shape</span><br><span class="line">(5000, 36)</span><br><span class="line"></span><br><span class="line">X_recover=X_reduction@U1.T+means</span><br></pre></td></tr></table></figure><h2 id="画图"><a href="#画图" class="headerlink" title="画图"></a>画图</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_100_image(X_recover),plot_100_image(X)</span><br></pre></td></tr></table></figure><p>可以看见，经过PCA降维处理，人脸图像变得模糊，丢失了许多细节，各图片的五官失去了一定的独特性。但是，人脸的大致轮廓得以保留。</p><p><img src="/2023/09/19/PCA/output_7_1.png" alt="png"></p><p><img src="/2023/09/19/PCA/output_7_2.png" alt="png"></p><h2 id="Site"><a href="#Site" class="headerlink" title="Site"></a>Site</h2><p>代码(Jupyter)和所用数据：<a href="https://github.com/codeYu233/Study/tree/main/PCA">https://github.com/codeYu233/Study/tree/main/PCA</a></p><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>该题与数据集均来源于Coursera上斯坦福大学的吴恩达老师机器学习的习题作业，学习交流用，如有不妥，立马删除</p>]]></content>
      
      
      <categories>
          
          <category> Study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Program </tag>
            
            <tag> ML </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kmeans_2</title>
      <link href="/2023/09/17/Kmeans_2/"/>
      <url>/2023/09/17/Kmeans_2/</url>
      
        <content type="html"><![CDATA[<h1 id="Kmeans-2"><a href="#Kmeans-2" class="headerlink" title="Kmeans_2"></a><em><strong>Kmeans_2</strong></em></h1><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><p>利用Kmeans算法，对目标图片进行减色处理。</p><h2 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h2><p>即将色彩丰富的一张图片根据像素点的相似程度，换成仅有16种颜色的图片。这种操作可以在保留图片基本特征的同时大大减少内存的占用，视觉上打折扣，但是在使用上方便很多。我们通过matlab将jpg，png格式的图片先转为mat格式的图片矩阵。再将图片矩阵转换为一个像素点一行的数据矩阵。接着进行常规的Kmeans操作，同文章Kmeans_1中所示，然后将聚类完毕的数据矩阵还原为图片矩阵，并进行显示。</p><h2 id="完整程序"><a href="#完整程序" class="headerlink" title="完整程序"></a>完整程序</h2><p>本代码这次没有用jupyter进行编写运行，而是选择了pycharm(因为jupyter的运行计算速度太吓人了，算个624×624的图片半天没反应，电脑风扇都没动静，无奈只能换成pycharm运行)。</p><p>注意，pycharm中plt.imshow()操作并不能直接显示图片，要在其操作后加上plt.show()。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import scipy.io as sio</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">data=sio.loadmat(&#x27;Kmeans_2.mat&#x27;)</span><br><span class="line">data.keys()</span><br><span class="line"></span><br><span class="line">A=data[&#x27;test&#x27;]</span><br><span class="line">A.shape</span><br><span class="line"></span><br><span class="line">A=A/255</span><br><span class="line">plt.imshow(A)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">A=A.reshape(-1,3)</span><br><span class="line">A.shape</span><br><span class="line"></span><br><span class="line">def find_centroids(X,centros):    </span><br><span class="line">    idx=[]    </span><br><span class="line">    for i in range(len(X)):</span><br><span class="line">        dist = np.linalg.norm((X[i]-centros),axis=1)</span><br><span class="line">        id_i=np.argmin(dist)</span><br><span class="line">        idx.append(id_i)        </span><br><span class="line">    return np.array(idx)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_centros(X,idx,k):    </span><br><span class="line">    centros=[] </span><br><span class="line">    for i in range(k):</span><br><span class="line">        centros_i=np.mean(X[idx==i],axis=0)</span><br><span class="line">        centros.append(centros_i)        </span><br><span class="line">    return np.array(centros)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def run_kmeans(X,centros,iters): </span><br><span class="line">    k=len(centros)</span><br><span class="line">    centros_all=[]</span><br><span class="line">    centros_all.append(centros)</span><br><span class="line">    centros_i=centros</span><br><span class="line">    for i in range(iters):</span><br><span class="line">        idx=find_centroids(X,centros_i)</span><br><span class="line">        centros_i=compute_centros(X,idx,k)</span><br><span class="line">        centros_all.append(centros_i)</span><br><span class="line">    return idx,np.array(centros_all)</span><br><span class="line"></span><br><span class="line">def init_centros(X,k):</span><br><span class="line">    index=np.random.choice(len(X),k)</span><br><span class="line">    return X[index]</span><br><span class="line"></span><br><span class="line">k=16</span><br><span class="line">idx,centros_all=run_kmeans(A,init_centros(A,k=16),iters=20)</span><br><span class="line">centros=centros_all[-1]</span><br><span class="line">im=np.zeros(A.shape)</span><br><span class="line">for i in range(k):</span><br><span class="line">    im[idx==i]=centros[i]</span><br><span class="line">im=im.reshape(642,642,3)</span><br><span class="line">print(im)</span><br><span class="line">plt.imshow(im)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="结果展示"><a href="#结果展示" class="headerlink" title="结果展示"></a>结果展示</h2><h3 id="减色前："><a href="#减色前：" class="headerlink" title="减色前："></a>减色前：</h3><p><img src="/2023/09/17/Kmeans_2/1.png" alt="png"></p><h3 id="减色后："><a href="#减色后：" class="headerlink" title="减色后："></a>减色后：</h3><p>可以看出来图片的基本特征得以保留，并且减色后图片去除了人物眼睛等部位的复杂颜色，统一换为了主题色：紫色。</p><p><img src="/2023/09/17/Kmeans_2/2.png" alt="png"></p><h2 id="Site"><a href="#Site" class="headerlink" title="Site"></a>Site</h2><p>代码(Python)和所用数据：<a href="https://github.com/codeYu233/Study/tree/main/Kmeans_2">https://github.com/codeYu233/Study/tree/main/Kmeans_2</a></p><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>该题与数据集均来源于Coursera上斯坦福大学的吴恩达老师机器学习的习题作业，学习交流用，如有不妥，立马删除</p>]]></content>
      
      
      <categories>
          
          <category> Study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Program </tag>
            
            <tag> ML </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kmeans_1</title>
      <link href="/2023/09/16/Kmeans_1/"/>
      <url>/2023/09/16/Kmeans_1/</url>
      
        <content type="html"><![CDATA[<h1 id="Kmeans-1"><a href="#Kmeans-1" class="headerlink" title="Kmeans_1"></a><em><strong>Kmeans_1</strong></em></h1><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><p>The <em>K</em>-means algorithm is a method to automatically cluster similar data examples together. Concretely, you are given a training set , and want to group the data into a few cohesive “clusters”.</p><h2 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h2><p>Kmeans属于无监督学习(Unsupervised Learning)中的一个方法，主要用于对已有数据进行分类以及确定每个聚类的中心位置，这样我们在获得一个新的未分类点时就可以对其进行预测，看它属于哪一个聚类。具体操作如下：选定k个聚点，然后针对各数据点分别对所有聚点求范数（求距离）。如果数据点a距离聚点A距离最小，则将a归类在A的聚类。对所有数据点进行这样的操作之后，我们就将数据点进行了第一次的分类。接着我们让聚点A的坐标更改为所有归类于A的聚类的平均坐标。即将A改为此时聚类的中心位置。对其他聚点也进行这样的操作。接着我们继续求范数，改坐标的操作。如果初始聚点位置选取合理，经过多次循环，我们就能得到最终的分类结果。</p><h2 id="数据读取处理"><a href="#数据读取处理" class="headerlink" title="数据读取处理"></a>数据读取处理</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import scipy.io as sio</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">data1=sio.loadmat(&#x27;./Kmeans_1.mat&#x27;)</span><br><span class="line"></span><br><span class="line">data1.keys()</span><br><span class="line">dict_keys([&#x27;__header__&#x27;, &#x27;__version__&#x27;, &#x27;__globals__&#x27;, &#x27;X&#x27;])</span><br><span class="line"></span><br><span class="line">X=data1[&#x27;X&#x27;]</span><br><span class="line">X.shape</span><br><span class="line">(300, 2)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2023/09/16/Kmeans_1/output_3_0.png" alt="png"></p><h2 id="数据点归类"><a href="#数据点归类" class="headerlink" title="数据点归类"></a>数据点归类</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def find_centroids(X,centros):</span><br><span class="line">    </span><br><span class="line">    idx=[]</span><br><span class="line">    </span><br><span class="line">    for i in range(len(X)):</span><br><span class="line">        dist = np.linalg.norm((X[i]-centros),axis=1)</span><br><span class="line">        id_i=np.argmin(dist)</span><br><span class="line">        idx.append(id_i)</span><br><span class="line">        </span><br><span class="line">    return np.array(idx)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">centros=np.array([[3,3],[6,2],[8,5]])</span><br><span class="line">idx=find_centroids(X,centros)</span><br><span class="line"></span><br><span class="line">idx[:3]</span><br><span class="line">array([0, 2, 1], dtype=int64)</span><br></pre></td></tr></table></figure><h2 id="移动聚点"><a href="#移动聚点" class="headerlink" title="移动聚点"></a>移动聚点</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def compute_centros(X,idx,k):</span><br><span class="line">    centros=[]</span><br><span class="line">    for i in range(k):</span><br><span class="line">        centros_i=np.mean(X[idx==i],axis=0)</span><br><span class="line">        centros.append(centros_i)</span><br><span class="line">        </span><br><span class="line">    return np.array(centros)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">compute_centros(X,idx,k=3)</span><br><span class="line">array([[2.42830111, 3.15792418],</span><br><span class="line">       [5.81350331, 2.63365645],</span><br><span class="line">       [7.11938687, 3.6166844 ]])</span><br></pre></td></tr></table></figure><h2 id="最终循环代码"><a href="#最终循环代码" class="headerlink" title="最终循环代码"></a>最终循环代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">run_kmeans</span>(<span class="params">X,centros,iters</span>):    </span><br><span class="line">    k=<span class="built_in">len</span>(centros)</span><br><span class="line">    centros_all=[]</span><br><span class="line">    centros_all.append(centros)</span><br><span class="line">    centros_i=centros</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iters):</span><br><span class="line">        idx=find_centroids(X,centros_i)</span><br><span class="line">        centros_i=compute_centros(X,idx,k)</span><br><span class="line">        centros_all.append(centros_i)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> idx,np.array(centros_all)</span><br></pre></td></tr></table></figure><h2 id="画图"><a href="#画图" class="headerlink" title="画图"></a>画图</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def plot_data(X,centros_all,idx):</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.scatter(X[:,0],X[:,1],c=idx,cmap=&#x27;rainbow&#x27;)</span><br><span class="line">    plt.plot(centros_all[:,:,0],centros_all[:,:,1],&#x27;kx--&#x27;)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">idx,centros_all=run_kmeans(X,centros,iters=10)</span><br><span class="line">plot_data(X,centros_all,idx)</span><br></pre></td></tr></table></figure><p><img src="/2023/09/16/Kmeans_1/output_11_0.png" alt="png"></p><p>下面我们随机选取3个数据点作为初始聚点，看看聚点的初始选择对结果的影响。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_centros</span>(<span class="params">X,k</span>):</span><br><span class="line">    index=np.random.choice(<span class="built_in">len</span>(X),k)</span><br><span class="line">    <span class="keyword">return</span> X[index]</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">init_centros(X,k=3)</span><br><span class="line">array([[3.91596068, 1.01225774],</span><br><span class="line">       [1.97619886, 4.43489674],</span><br><span class="line">       [5.72395697, 3.04454219]])</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for i in range(4):</span><br><span class="line">    idx,centros_all=run_kmeans(X,init_centros(X,k=3),iters=10)</span><br><span class="line">    plot_data(X,centros_all,idx)</span><br></pre></td></tr></table></figure><p>由第一幅图可以看见，当我们初始聚点为这样时，最终的分类效果并不令人满意，它将一体的数据强行分为了两块，将两块分离的数据强行合在了一起。</p><p><img src="/2023/09/16/Kmeans_1/output_14_0.png" alt="png"></p><p><img src="/2023/09/16/Kmeans_1/output_14_1.png" alt="png"></p><p><img src="/2023/09/16/Kmeans_1/output_14_2.png" alt="png"></p><p><img src="/2023/09/16/Kmeans_1/output_14_3.png" alt="png"></p><h2 id="Site"><a href="#Site" class="headerlink" title="Site"></a>Site</h2><p>代码(Jupyter)和所用数据：<a href="https://github.com/codeYu233/Study/tree/main/Kmeans_1">https://github.com/codeYu233/Study/tree/main/Kmeans_1</a></p><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>该题与数据集均来源于Coursera上斯坦福大学的吴恩达老师机器学习的习题作业，学习交流用，如有不妥，立马删除</p>]]></content>
      
      
      <categories>
          
          <category> Study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Program </tag>
            
            <tag> ML </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bias and Variance</title>
      <link href="/2023/09/11/Bias%20and%20Variance/"/>
      <url>/2023/09/11/Bias%20and%20Variance/</url>
      
        <content type="html"><![CDATA[<h1 id="Bias-and-Variance"><a href="#Bias-and-Variance" class="headerlink" title="Bias and Variance"></a><em><strong>Bias and Variance</strong></em></h1><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><p>In the first half of the exercise, you will implement regularized linear regression to predict the amount of water flowing out of a dam using the change of water level in a reservoir. In the next half, you will go through some diagnostics of debugging learning algorithms and examine the effects of bias and variance.</p><h2 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h2><p>我们在训练模型时通常准备训练集，验证集和测试集三组数据。训练集负责提供数据进行训练，验证集负责对训练结果进行评价并反馈修改意见，测试集体现最终的一个预测能力。偏差主要体现为训练后模型与训练集中数据的偏离程度，偏差越小代表与训练集数据拟合越好（太小会导致过拟合）。方差体现为训练后模型加入验证集数据后，预测结果与验证集数据的偏离程度，我们也需要使方差在一定程度上尽可能小，使得我们测试集中的预测结果不会偏离真实结果太多。这里X将从最基本的一次线性函数开始，通过绘图直观展示特征化、归一化、正则化、增加数据数量等手段对于模型及其偏差、方差的影响。</p><h2 id="数据读取处理"><a href="#数据读取处理" class="headerlink" title="数据读取处理"></a>数据读取处理</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from scipy.io import loadmat</span><br><span class="line">from scipy.optimize import minimize</span><br><span class="line"></span><br><span class="line">data=loadmat(&#x27;Bias and Variance.mat&#x27;)</span><br><span class="line">data.keys()</span><br><span class="line">dict_keys([&#x27;__header__&#x27;, &#x27;__version__&#x27;, &#x27;__globals__&#x27;, &#x27;X&#x27;, &#x27;y&#x27;, &#x27;Xtest&#x27;, &#x27;ytest&#x27;, &#x27;Xval&#x27;, &#x27;yval&#x27;])</span><br><span class="line"></span><br><span class="line">X_train,y_train=data[&#x27;X&#x27;],data[&#x27;y&#x27;]</span><br><span class="line">X_train.shape,y_train.shape</span><br><span class="line">((12, 1), (12, 1))</span><br><span class="line"></span><br><span class="line">X_val,y_val=data[&#x27;Xval&#x27;],data[&#x27;yval&#x27;]</span><br><span class="line">X_val.shape,y_val.shape</span><br><span class="line">((21, 1), (21, 1))</span><br><span class="line"></span><br><span class="line">X_test,y_test=data[&#x27;Xtest&#x27;],data[&#x27;ytest&#x27;]</span><br><span class="line">X_test.shape,y_test.shape</span><br><span class="line">((21, 1), (21, 1))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_train=np.insert(X_train,<span class="number">0</span>,<span class="number">1</span>,axis=<span class="number">1</span>)</span><br><span class="line">X_val=np.insert(X_val,<span class="number">0</span>,<span class="number">1</span>,axis=<span class="number">1</span>)</span><br><span class="line">X_test=np.insert(X_test,<span class="number">0</span>,<span class="number">1</span>,axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_data</span>():</span><br><span class="line">    fig,ax=plt.subplots()</span><br><span class="line">    ax.scatter(X_train[:,<span class="number">1</span>],y_train)</span><br><span class="line">    ax.<span class="built_in">set</span>(xlabel=<span class="string">&#x27;change in water level(x)&#x27;</span>,ylabel=<span class="string">&#x27;water flowing out of the dam(y)&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_data()</span><br></pre></td></tr></table></figure><p>​<br><img src="/2023/09/11/Bias%20and%20Variance/1.png" alt="png"><br>​    </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reg_cost</span>(<span class="params">theta,X,y,lamda</span>):</span><br><span class="line">    </span><br><span class="line">    cost=np.<span class="built_in">sum</span>(np.power((X@theta-y.flatten()),<span class="number">2</span>))</span><br><span class="line">    reg=theta[<span class="number">1</span>:]@theta[<span class="number">1</span>:]*lamda</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> (cost+reg)/(<span class="number">2</span>*<span class="built_in">len</span>(X))</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">theta=np.ones(X_train.shape[1])</span><br><span class="line">lamda=1</span><br><span class="line">reg_cost(theta,X_train,y_train,lamda)</span><br><span class="line">303.9931922202643</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reg_gradient</span>(<span class="params">theta,X,y,lamda</span>):</span><br><span class="line">    </span><br><span class="line">    grad=(X@theta-y.flatten())@X</span><br><span class="line">    reg=lamda*theta</span><br><span class="line">    reg[<span class="number">0</span>]=<span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> (grad+reg)/<span class="built_in">len</span>(X)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">reg_gradient(theta,X_train,y_train,lamda)</span><br><span class="line">array([-15.30301567, 598.25074417])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>(<span class="params">X,y,lamda</span>):</span><br><span class="line">    </span><br><span class="line">    theta=np.ones(X.shape[<span class="number">1</span>])</span><br><span class="line">    res=minimize(fun=reg_cost,x0=theta,args=(X,y,lamda),method=<span class="string">&#x27;TNC&#x27;</span>,jac=reg_gradient)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> res.x</span><br></pre></td></tr></table></figure><h2 id="普通的一次线性"><a href="#普通的一次线性" class="headerlink" title="普通的一次线性"></a>普通的一次线性</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">theta_final=train_model(X_train,y_train,lamda=0)</span><br><span class="line"></span><br><span class="line">plot_data()</span><br><span class="line">plt.plot(X_train[:,1],X_train@theta_final,c=&#x27;r&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2023/09/11/Bias%20and%20Variance/2.png" alt="png"></p><h2 id="偏差方差随数据增多的变化图"><a href="#偏差方差随数据增多的变化图" class="headerlink" title="偏差方差随数据增多的变化图"></a>偏差方差随数据增多的变化图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_learning_curve</span>(<span class="params">X_train,y_train,X_val,y_val,lamda</span>):</span><br><span class="line">    </span><br><span class="line">    x=<span class="built_in">range</span>(<span class="number">1</span>,<span class="built_in">len</span>(X_train)+<span class="number">1</span>)</span><br><span class="line">    training_cost=[]</span><br><span class="line">    cv_cost=[]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> x:</span><br><span class="line">        res=train_model(X_train[:i,:],y_train[:i,:],lamda)</span><br><span class="line">        training_cost_i=reg_cost(res,X_train[:i,:],y_train[:i,:],lamda)</span><br><span class="line">        cv_cost_i=reg_cost(res,X_val,y_val,lamda)</span><br><span class="line">        training_cost.append(training_cost_i)</span><br><span class="line">        cv_cost.append(cv_cost_i)</span><br><span class="line">        </span><br><span class="line">    plt.plot(x,training_cost,label=<span class="string">&#x27;training cost&#x27;</span>)</span><br><span class="line">    plt.plot(x,cv_cost,label=<span class="string">&#x27;cv cost&#x27;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;number of training examples&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;error&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_learning_curve(X_train,y_train,X_val,y_val,<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>​<br><img src="/2023/09/11/Bias%20and%20Variance/3.png" alt="png"><br>​    </p><h2 id="将X特征化和归一化"><a href="#将X特征化和归一化" class="headerlink" title="将X特征化和归一化"></a>将X特征化和归一化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">poly_feature</span>(<span class="params">X,power</span>):</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>,power+<span class="number">1</span>):</span><br><span class="line">        X=np.insert(X,X.shape[<span class="number">1</span>],np.power(X[:,<span class="number">1</span>],i),axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_means_stds</span>(<span class="params">X</span>):</span><br><span class="line">    </span><br><span class="line">    means=np.mean(X,axis=<span class="number">0</span>)</span><br><span class="line">    stds=np.std(X,axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> means,stds</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">feature_normalize</span>(<span class="params">X,means,stds</span>):</span><br><span class="line">    </span><br><span class="line">    X[:,<span class="number">1</span>:]=(X[:,<span class="number">1</span>:]-means[<span class="number">1</span>:])/stds[<span class="number">1</span>:]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">power=<span class="number">6</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_train_poly=poly_feature(X_train,power)</span><br><span class="line">X_val_poly=poly_feature(X_val,power)</span><br><span class="line">X_test_poly=poly_feature(X_test,power)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_means,train_stds=get_means_stds(X_train_poly)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_train_norm=feature_normalize(X_train_poly,train_means,train_stds)</span><br><span class="line">X_val_norm=feature_normalize(X_val_poly,train_means,train_stds)</span><br><span class="line">X_test_norm=feature_normalize(X_test_poly,train_means,train_stds)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theta_fit=train_model(X_train_norm,y_train,lamda=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_poly_fit</span>():</span><br><span class="line">    plot_data()</span><br><span class="line">    x=np.linspace(-<span class="number">60</span>,<span class="number">60</span>,<span class="number">100</span>)</span><br><span class="line">    xx=x.reshape(<span class="number">100</span>,<span class="number">1</span>)</span><br><span class="line">    xx=np.insert(xx,<span class="number">0</span>,<span class="number">1</span>,axis=<span class="number">1</span>)</span><br><span class="line">    xx=poly_feature(xx,power)</span><br><span class="line">    xx=feature_normalize(xx,train_means,train_stds)</span><br><span class="line">    </span><br><span class="line">    plt.plot(x,xx@theta_fit,<span class="string">&#x27;r--&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_poly_fit()</span><br></pre></td></tr></table></figure><p>​<br><img src="/2023/09/11/Bias%20and%20Variance/4.png" alt="png"><br>​    </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_learning_curve(X_train_norm,y_train,X_val_norm,y_val,lamda=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>​<br><img src="/2023/09/11/Bias%20and%20Variance/5.png" alt="png"><br>​    </p><p>这里发现随数据的增多偏差恒为0，且模型最终曲线具有特殊性，属于过拟合的情况，加入正则化手段防止其过拟合。</p><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>lambda&#x3D;1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_learning_curve(X_train_norm,y_train,X_val_norm,y_val,lamda=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>​<br><img src="/2023/09/11/Bias%20and%20Variance/6.png" alt="png"><br>​    </p><p>lambda&#x3D;100</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_learning_curve(X_train_norm,y_train,X_val_norm,y_val,lamda=<span class="number">100</span>)</span><br></pre></td></tr></table></figure><p>​<br><img src="/2023/09/11/Bias%20and%20Variance/7.png" alt="png"><br>​    </p><h3 id="为正则化找到最合适的lambda"><a href="#为正则化找到最合适的lambda" class="headerlink" title="为正则化找到最合适的lambda"></a>为正则化找到最合适的lambda</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">lamdas=[<span class="number">0</span>,<span class="number">0.001</span>,<span class="number">0.003</span>,<span class="number">0.01</span>,<span class="number">0.03</span>,<span class="number">0.1</span>,<span class="number">0.3</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">10</span>]</span><br><span class="line"></span><br><span class="line">training_cost=[]</span><br><span class="line">cv_cost=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> lamda <span class="keyword">in</span> lamdas:</span><br><span class="line">    res=train_model(X_train_norm,y_train,lamda)</span><br><span class="line">    </span><br><span class="line">    tc=reg_cost(res,X_train_norm,y_train,lamda=<span class="number">0</span>)</span><br><span class="line">    cv=reg_cost(res,X_val_norm,y_val,lamda=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    training_cost.append(tc)</span><br><span class="line">    cv_cost.append(cv)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(lamdas,training_cost,label=<span class="string">&#x27;training cost&#x27;</span>)</span><br><span class="line">plt.plot(lamdas,cv_cost,label=<span class="string">&#x27;cv cost&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>​<br><img src="/2023/09/11/Bias%20and%20Variance/8.png" alt="png"><br>​    </p><p>方差最小对应的lambda为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lamdas[np.argmin(cv_cost)]</span><br><span class="line">3</span><br></pre></td></tr></table></figure><p>根据图像，将此时的lambda视为最优参数，最终测试集的损失函数值为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">res=train_model(X_train_norm,y_train,lamda=3)</span><br><span class="line">test_cost=reg_cost(res,X_test_norm,y_test,lamda=0)</span><br><span class="line">print(test_cost)</span><br><span class="line"></span><br><span class="line">4.3976161577441975</span><br></pre></td></tr></table></figure><h2 id="Site"><a href="#Site" class="headerlink" title="Site"></a>Site</h2><p>代码(Jupyter)和所用数据：<a href="https://github.com/codeYu233/Study/tree/main/Bias%20and%20Variance">https://github.com/codeYu233/Study/tree/main/Bias%20and%20Variance</a></p><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>该题与数据集均来源于Coursera上斯坦福大学的吴恩达老师机器学习的习题作业，学习交流用，如有不妥，立马删除</p>]]></content>
      
      
      <categories>
          
          <category> Study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Program </tag>
            
            <tag> ML </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Logistic Regression_3（更有趣的逻辑回归）</title>
      <link href="/2023/09/09/Logistic%20Regression_3/"/>
      <url>/2023/09/09/Logistic%20Regression_3/</url>
      
        <content type="html"><![CDATA[<h1 id="Logistic-Regression-3"><a href="#Logistic-Regression-3" class="headerlink" title="Logistic Regression_3"></a><em><strong>Logistic Regression_3</strong></em></h1><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><p>For this exercise, you will use logistic regression and neural networks to recognize handwritten digits (from 0 to 9). Automated handwritten digit recognition is widely used today - from recognizing zip codes (postal codes) on mail envelopes to recognizing amounts written on bank checks. This exercise will show you how the methods you’ve learned can be used for this classification task. In the first part of the exercise, you will extend your previous implementation of logistic regression and apply it to one-vs-all classification.</p><h2 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h2><p>底层理论跟普通的逻辑回归没什么不同，只是分类的标签变成了10个，一个图片仅拥有一个标签。像这样的数字或字母识别我们生活中经常用到。（当然更多情况是一个物体对应很多标签，要进行那样的智能识别需要较大的数据和计算，经常用神经网络去完成）对于本题采用逻辑回归的分类，我们只需要将之前的向量Theta变为列数为10的向量组即可。进行训练时，训练集中图片所示数字对应的y值设置为1就行(图片为3，则y_3&#x3D;1，其余为0)。最终X与Theta相乘能得到一个长度为10的结果数组。每个数各代表对应序号的数字的可能性大小，取数组中最大的数即可作为模型预测结果。(本文将1对应于序号1，2对应于序号2，….，但0对应于序号10)<br>$$<br>\Theta&#x3D;\begin{bmatrix}<br> \Theta_{1} &amp; \Theta_{2} &amp; … &amp; \Theta_{10}<br>\end{bmatrix}\\<br>$$</p><p>$$<br>\Theta_{i}&#x3D;\begin{bmatrix}\theta_{0}^{i}\\<br>\theta_{1}^{i}\\<br>…\\<br>\theta_{400}^{i}<br>\end{bmatrix}<br>$$</p><p>400是因为本题所给图片以20×20的数据形式保存，拉长成一维数组后有400个数据，外加一个“常数项” theta_0，Theta中一列共401个数据。</p><h2 id="数据读取处理"><a href="#数据读取处理" class="headerlink" title="数据读取处理"></a>数据读取处理</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import scipy.io as sio</span><br><span class="line"></span><br><span class="line">data = sio.loadmat(&#x27;Logistic Regression_3.mat&#x27;)</span><br><span class="line">data</span><br></pre></td></tr></table></figure><pre><code>&#123;&#39;__header__&#39;: b&#39;MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011&#39;, &#39;__version__&#39;: &#39;1.0&#39;, &#39;__globals__&#39;: [], &#39;X&#39;: array([[0., 0., 0., ..., 0., 0., 0.],        [0., 0., 0., ..., 0., 0., 0.],        [0., 0., 0., ..., 0., 0., 0.],        ...,        [0., 0., 0., ..., 0., 0., 0.],        [0., 0., 0., ..., 0., 0., 0.],        [0., 0., 0., ..., 0., 0., 0.]]), &#39;y&#39;: array([[10],        [10],        [10],        ...,        [ 9],        [ 9],        [ 9]], dtype=uint8)&#125;</code></pre><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">type(data)</span><br><span class="line">dict</span><br><span class="line"></span><br><span class="line">data.keys()</span><br><span class="line">dict_keys([&#x27;__header__&#x27;, &#x27;__version__&#x27;, &#x27;__globals__&#x27;, &#x27;X&#x27;, &#x27;y&#x27;])</span><br><span class="line"></span><br><span class="line">raw_X=data[&#x27;X&#x27;]</span><br><span class="line">raw_y=data[&#x27;y&#x27;]</span><br><span class="line"></span><br><span class="line">print(raw_X.shape,raw_y.shape)</span><br><span class="line">(5000, 400) (5000, 1)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def plot_an_image(X):</span><br><span class="line">    </span><br><span class="line">    pick_one=np.random.randint(5000)</span><br><span class="line">    image=X[pick_one,:]</span><br><span class="line">    fig,ax=plt.subplots(figsize=(1,1))</span><br><span class="line">    ax.imshow(image.reshape(20,20).T,cmap=&#x27;gray_r&#x27;)</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_an_image(raw_X)</span><br></pre></td></tr></table></figure><p><img src="/2023/09/09/Logistic%20Regression_3/1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_100_image</span>(<span class="params">X</span>):</span><br><span class="line">    </span><br><span class="line">    sample_index=np.random.choice(<span class="built_in">len</span>(X),<span class="number">100</span>)</span><br><span class="line">    images=X[sample_index,:]</span><br><span class="line">    <span class="built_in">print</span>(images.shape)</span><br><span class="line">    </span><br><span class="line">    fig,ax=plt.subplots(ncols=<span class="number">10</span>,nrows=<span class="number">10</span>,figsize=(<span class="number">8</span>,<span class="number">8</span>,),sharex=<span class="literal">True</span>,sharey=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">            ax[r,c].imshow(images[<span class="number">10</span>*r+c].reshape(<span class="number">20</span>,<span class="number">20</span>).T,cmap=<span class="string">&#x27;gray_r&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])</span><br><span class="line">    plt.show</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_100_image(raw_X)</span><br><span class="line">(100, 400)</span><br></pre></td></tr></table></figure><p><img src="/2023/09/09/Logistic%20Regression_3/2.png" alt="png"></p><h2 id="损失函数与梯度下降"><a href="#损失函数与梯度下降" class="headerlink" title="损失函数与梯度下降"></a>损失函数与梯度下降</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">costFunction</span>(<span class="params">theta,X,y,lamda</span>):</span><br><span class="line">    </span><br><span class="line">    first=y*np.log(sigmoid(X@theta))</span><br><span class="line">    second=(<span class="number">1</span>-y)*np.log(<span class="number">1</span>-sigmoid(X@theta))</span><br><span class="line">    </span><br><span class="line">    reg=theta[<span class="number">1</span>:]@theta[<span class="number">1</span>:]*(lamda/(<span class="number">2</span>*<span class="built_in">len</span>(X)))</span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(first+second)/<span class="built_in">len</span>(X)+reg</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_reg</span>(<span class="params">theta,X,y,lamda</span>):</span><br><span class="line">    reg=theta[<span class="number">1</span>:]*(lamda/<span class="built_in">len</span>(X))</span><br><span class="line">    reg=np.insert(reg,<span class="number">0</span>,values=<span class="number">0</span>,axis=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    first=(X.T@(sigmoid(X@theta)-y))/<span class="built_in">len</span>(X)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> first+reg</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X=np.insert(raw_X,0,values=1,axis=1)</span><br><span class="line">X.shape</span><br><span class="line">(5000, 401)</span><br><span class="line">y=raw_y.flatten()</span><br><span class="line">y.shape</span><br><span class="line">(5000,)</span><br></pre></td></tr></table></figure><h2 id="模型生成"><a href="#模型生成" class="headerlink" title="模型生成"></a>模型生成</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">one_vs_all</span>(<span class="params">X,y,lamda,K</span>):</span><br><span class="line">    </span><br><span class="line">    n=X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    theta_all=np.zeros((K,n))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,K+<span class="number">1</span>):</span><br><span class="line">        theta_i=np.zeros(n,)</span><br><span class="line">        </span><br><span class="line">        res=minimize(fun=costFunction,x0=theta_i,args=(X,y==i,lamda),method=<span class="string">&#x27;TNC&#x27;</span>,jac=gradient_reg)</span><br><span class="line">        theta_all[i-<span class="number">1</span>,:]=res.x</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> theta_all</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lamda=1</span><br><span class="line">K=10</span><br><span class="line">theta_final=one_vs_all(X,y,lamda,K)</span><br><span class="line">theta_final</span><br></pre></td></tr></table></figure><pre><code>array([[-2.38187334e+00,  0.00000000e+00,  0.00000000e+00, ...,         1.30433279e-03, -7.29580949e-10,  0.00000000e+00],       [-3.18303389e+00,  0.00000000e+00,  0.00000000e+00, ...,         4.46340729e-03, -5.08870029e-04,  0.00000000e+00],       [-4.79638233e+00,  0.00000000e+00,  0.00000000e+00, ...,        -2.87468695e-05, -2.47395863e-07,  0.00000000e+00],       ...,       [-7.98700752e+00,  0.00000000e+00,  0.00000000e+00, ...,        -8.94576566e-05,  7.21256372e-06,  0.00000000e+00],       [-4.57358931e+00,  0.00000000e+00,  0.00000000e+00, ...,        -1.33390955e-03,  9.96868542e-05,  0.00000000e+00],       [-5.40542751e+00,  0.00000000e+00,  0.00000000e+00, ...,        -1.16613537e-04,  7.88124085e-06,  0.00000000e+00]])</code></pre><h2 id="预测结果准确率"><a href="#预测结果准确率" class="headerlink" title="预测结果准确率"></a>预测结果准确率</h2><p>将预测结果1的位置与图片实际的数字序号（1）的位置进行比对，相同则记1，不同则记0。将结果累加并除以测试用数据个数，即可得到准确率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">X,theta_final</span>):</span><br><span class="line">    </span><br><span class="line">    h=sigmoid(X@theta_final.T)</span><br><span class="line">    </span><br><span class="line">    h_argmax=np.argmax(h,axis=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> h_argmax+<span class="number">1</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y_pred=predict(X,theta_final)</span><br><span class="line">acc=np.mean(y_pred==y)</span><br><span class="line">acc</span><br><span class="line">0.9446</span><br></pre></td></tr></table></figure><h2 id="实际图片与预测结果的比对"><a href="#实际图片与预测结果的比对" class="headerlink" title="实际图片与预测结果的比对"></a>实际图片与预测结果的比对</h2><p>该模型用训练集进行检测准确率为0.9446，从下面随机选取的10个样本可以看见，该模型错把一个5认成了9。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_an_image_test</span>(<span class="params">X,X_2</span>):</span><br><span class="line">    </span><br><span class="line">    pick_test=np.random.randint(<span class="number">5000</span>)</span><br><span class="line">    </span><br><span class="line">    image=X[pick_test,:]</span><br><span class="line">    </span><br><span class="line">    fig,ax=plt.subplots(figsize=(<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">    ax.imshow(image.reshape(<span class="number">20</span>,<span class="number">20</span>).T,cmap=<span class="string">&#x27;gray_r&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    test_result=sigmoid(X_2[pick_test,:]@theta_final.T)</span><br><span class="line">    test_answer=np.argmax(test_result)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;数字判断为&quot;</span>,(<span class="number">0</span> <span class="keyword">if</span> test_answer+<span class="number">1</span>==<span class="number">10</span> <span class="keyword">else</span> test_answer+<span class="number">1</span>),<span class="string">f&quot;实际为&quot;</span>,(<span class="number">0</span> <span class="keyword">if</span> raw_y[pick_test,<span class="number">0</span>]==<span class="number">10</span> <span class="keyword">else</span> raw_y[pick_test,<span class="number">0</span>]))</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">for i in range(10): </span><br><span class="line">    plot_an_image_test(raw_X,X)</span><br><span class="line"></span><br><span class="line">数字判断为 1 实际为 1</span><br><span class="line">数字判断为 2 实际为 2</span><br><span class="line">数字判断为 3 实际为 3</span><br><span class="line">数字判断为 9 实际为 9</span><br><span class="line">数字判断为 8 实际为 8</span><br><span class="line">数字判断为 9 实际为 5</span><br><span class="line">数字判断为 4 实际为 4</span><br><span class="line">数字判断为 3 实际为 3</span><br><span class="line">数字判断为 2 实际为 2</span><br><span class="line">数字判断为 7 实际为 7</span><br></pre></td></tr></table></figure><p><img src="/2023/09/09/Logistic%20Regression_3/3.png" alt="png"></p><p><img src="/2023/09/09/Logistic%20Regression_3/4.png" alt="png"></p><p><img src="/2023/09/09/Logistic%20Regression_3/5.png" alt="png"></p><p><img src="/2023/09/09/Logistic%20Regression_3/6.png" alt="png"></p><p><img src="/2023/09/09/Logistic%20Regression_3/7.png" alt="png"></p><p><img src="/2023/09/09/Logistic%20Regression_3/8.png" alt="png"></p><p><img src="/2023/09/09/Logistic%20Regression_3/9.png" alt="png"></p><p><img src="/2023/09/09/Logistic%20Regression_3/10.png" alt="png"></p><p><img src="/2023/09/09/Logistic%20Regression_3/11.png" alt="png"></p><p><img src="/2023/09/09/Logistic%20Regression_3/12.png" alt="png"></p><h2 id="Site"><a href="#Site" class="headerlink" title="Site"></a>Site</h2><p>代码(Jupyter)和所用数据：<a href="https://github.com/codeYu233/Study/tree/main/Logistic%20Regression_3">https://github.com/codeYu233/Study/tree/main/Logistic%20Regression_3</a></p><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>该题与数据集均来源于Coursera上斯坦福大学的吴恩达老师机器学习的习题作业，学习交流用，如有不妥，立马删除</p>]]></content>
      
      
      <categories>
          
          <category> Study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Program </tag>
            
            <tag> ML </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Logistic Regression_2（有趣的逻辑回归）</title>
      <link href="/2023/09/08/Logistic%20Regression_2/"/>
      <url>/2023/09/08/Logistic%20Regression_2/</url>
      
        <content type="html"><![CDATA[<h1 id="Logistic-Regression-2"><a href="#Logistic-Regression-2" class="headerlink" title="Logistic Regression_2"></a><em><strong>Logistic Regression_2</strong></em></h1><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><p>Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. From these two tests, you would like to determine whether the microchips should be accepted or rejected.</p><h2 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h2><p>跟文章Logistic Regression_1中的思路一样，我们需要进行分类预测。最简单的逻辑回归只需画一条线就能完成分类，这工作交给人来也能完成，并没有怎么很好体现机器学习的作用（如上一篇所示）。但是当我们导入工厂数据并绘制散点图后会发现事情并不是画一条线那么简单了。</p><p><img src="/2023/09/08/Logistic%20Regression_2/1.png" alt="png"></p><p>这次我们初步估计需要画一个椭圆来进行分界。一次的线性方程已经不能满足需求，我们需要将f(X)扩展到更高次。以本题需用到的六次为例：<br>$$<br>X&#x3D; \begin{bmatrix}<br>1\\<br>x_{1}\\<br>x_{2}\\<br>x_{1}^{2}\\<br>x_{2}^{2}\\<br>x_{1}x_{2}\\<br>x_{1}^{3}\\<br>…\\<br>x_{1}x_{2}^{5}\\<br>x_{2}^{6}<br>\end{bmatrix}<br>$$<br>我们根据牛顿的二项式定理可以在程序中实现对向量X的创建。然后就是跟往常一样的设置参数向量Theta，梯度下降，迭代学习。但是，对于高次的条件，最终结果很有可能出现过拟合的情况。最终的函数可能会以特殊的形状去很好地满足我们的条件。这样的函数很显然不能用来反应一般规律，不能拿来预测。<br><img src="/2023/09/08/Logistic%20Regression_2/3.png" alt="png"></p><p>因此，我们需要采取手段来避免这种过拟合。最简单的方法当然是获取更多的训练数据，只要数据够多，就足以能反应潜在规律，拟合后的函数也就越精确。但现实我们肯定不容易得到那么多的数据。因此我们需要对参数向量Theta进行一定的约束操作。参数越多，参数越大越杂，过拟合越容易发生，因此我们可以引入一个惩罚项加入损失函数来完成对参数大小的限制，从而避免过度拟合。这也叫做正则化，这里采用的是L2正则。<br>$$<br>\frac{\lambda }{2m}\sum_{j&#x3D;1}^{n}\theta_{j}^{2}<br>$$<br>因此损失函数成为了<br>$$<br>J(\Theta)&#x3D; - \frac{1}{m} \sum_{i}^{m} (y_{i} ln{}^{p(X)} + (1 - y_{i}) ln{}^{(1-p(X))})+\frac{\lambda }{2m}\sum_{j&#x3D;1}^{n}\theta_{j}^{2}<br>$$<br>注意惩罚项我们没有加入作为“常数项”的theta_0，因为theta_0在函数图上只表现为整体图像的平移，不会对图形的模样产生影响。因此梯度下降也需要分为theta_0和theta_j来进行。<br>$$<br>\theta_{j}&#x3D;\theta_{j}-(\frac{\alpha}{m}\sum_{i&#x3D;1}^{m}(p(x^{i})-y^{i})x_{j}^{i})\quad (j&#x3D;0)\\<br>\theta_{j}&#x3D;\theta_{j}-(\frac{\alpha}{m}\sum_{i&#x3D;1}^{m}(p(x^{i})-y^{i})x_{j}^{i}+\frac{\lambda}{m}\theta_{j})<br>$$<br>接下来只需编写代码正常操作即可</p><h2 id="数据读取处理"><a href="#数据读取处理" class="headerlink" title="数据读取处理"></a>数据读取处理</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">path=&#x27;Logistic Regression_2.txt&#x27;</span><br><span class="line">data=pd.read_csv(path,names=[&#x27;Test1&#x27;,&#x27;Test2&#x27;,&#x27;Accepted&#x27;])</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Test1</th>      <th>Test2</th>      <th>Accepted</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0.051267</td>      <td>0.69956</td>      <td>1</td>    </tr>    <tr>      <th>1</th>      <td>-0.092742</td>      <td>0.68494</td>      <td>1</td>    </tr>    <tr>      <th>2</th>      <td>-0.213710</td>      <td>0.69225</td>      <td>1</td>    </tr>    <tr>      <th>3</th>      <td>-0.375000</td>      <td>0.50219</td>      <td>1</td>    </tr>    <tr>      <th>4</th>      <td>-0.513250</td>      <td>0.46564</td>      <td>1</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fig,ax=plt.subplots()</span><br><span class="line">ax.scatter(data[data[<span class="string">&#x27;Accepted&#x27;</span>]==<span class="number">0</span>][<span class="string">&#x27;Test1&#x27;</span>],data[data[<span class="string">&#x27;Accepted&#x27;</span>]==<span class="number">0</span>][<span class="string">&#x27;Test2&#x27;</span>],c=<span class="string">&#x27;r&#x27;</span>,marker=<span class="string">&#x27;x&#x27;</span>,label=<span class="string">&#x27;y=0&#x27;</span>)</span><br><span class="line">ax.scatter(data[data[<span class="string">&#x27;Accepted&#x27;</span>]==<span class="number">1</span>][<span class="string">&#x27;Test1&#x27;</span>],data[data[<span class="string">&#x27;Accepted&#x27;</span>]==<span class="number">1</span>][<span class="string">&#x27;Test2&#x27;</span>],c=<span class="string">&#x27;b&#x27;</span>,marker=<span class="string">&#x27;o&#x27;</span>,label=<span class="string">&#x27;y=1&#x27;</span>)</span><br><span class="line">ax.legend()</span><br><span class="line">ax.<span class="built_in">set</span>(xlabel=<span class="string">&#x27;Test1&#x27;</span>,ylabel=<span class="string">&#x27;Test2&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>​<br><img src="/2023/09/08/Logistic%20Regression_2/1.png" alt="png"><br>​    </p><h2 id="特征映射"><a href="#特征映射" class="headerlink" title="特征映射"></a>特征映射</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">feature_mapping</span>(<span class="params">x1,x2,power</span>):</span><br><span class="line">    data=&#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> np.arange(power+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> np.arange(i+<span class="number">1</span>):</span><br><span class="line">            data[<span class="string">&#x27;F&#123;&#125;&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i-j,j)]=np.power(x1,i-j)*np.power(x2,j)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame(data)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x1=data[&#x27;Test1&#x27;]</span><br><span class="line">x2=data[&#x27;Test2&#x27;]</span><br><span class="line"></span><br><span class="line">data2=feature_mapping(x1,x2,6)</span><br><span class="line"></span><br><span class="line">data2.head()</span><br></pre></td></tr></table></figure><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>F00</th>      <th>F10</th>      <th>F01</th>      <th>F20</th>      <th>F11</th>      <th>F02</th>      <th>F30</th>      <th>F21</th>      <th>F12</th>      <th>F03</th>      <th>...</th>      <th>F23</th>      <th>F14</th>      <th>F05</th>      <th>F60</th>      <th>F51</th>      <th>F42</th>      <th>F33</th>      <th>F24</th>      <th>F15</th>      <th>F06</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1.0</td>      <td>0.051267</td>      <td>0.69956</td>      <td>0.002628</td>      <td>0.035864</td>      <td>0.489384</td>      <td>0.000135</td>      <td>0.001839</td>      <td>0.025089</td>      <td>0.342354</td>      <td>...</td>      <td>0.000900</td>      <td>0.012278</td>      <td>0.167542</td>      <td>1.815630e-08</td>      <td>2.477505e-07</td>      <td>0.000003</td>      <td>0.000046</td>      <td>0.000629</td>      <td>0.008589</td>      <td>0.117206</td>    </tr>    <tr>      <th>1</th>      <td>1.0</td>      <td>-0.092742</td>      <td>0.68494</td>      <td>0.008601</td>      <td>-0.063523</td>      <td>0.469143</td>      <td>-0.000798</td>      <td>0.005891</td>      <td>-0.043509</td>      <td>0.321335</td>      <td>...</td>      <td>0.002764</td>      <td>-0.020412</td>      <td>0.150752</td>      <td>6.362953e-07</td>      <td>-4.699318e-06</td>      <td>0.000035</td>      <td>-0.000256</td>      <td>0.001893</td>      <td>-0.013981</td>      <td>0.103256</td>    </tr>    <tr>      <th>2</th>      <td>1.0</td>      <td>-0.213710</td>      <td>0.69225</td>      <td>0.045672</td>      <td>-0.147941</td>      <td>0.479210</td>      <td>-0.009761</td>      <td>0.031616</td>      <td>-0.102412</td>      <td>0.331733</td>      <td>...</td>      <td>0.015151</td>      <td>-0.049077</td>      <td>0.158970</td>      <td>9.526844e-05</td>      <td>-3.085938e-04</td>      <td>0.001000</td>      <td>-0.003238</td>      <td>0.010488</td>      <td>-0.033973</td>      <td>0.110047</td>    </tr>    <tr>      <th>3</th>      <td>1.0</td>      <td>-0.375000</td>      <td>0.50219</td>      <td>0.140625</td>      <td>-0.188321</td>      <td>0.252195</td>      <td>-0.052734</td>      <td>0.070620</td>      <td>-0.094573</td>      <td>0.126650</td>      <td>...</td>      <td>0.017810</td>      <td>-0.023851</td>      <td>0.031940</td>      <td>2.780914e-03</td>      <td>-3.724126e-03</td>      <td>0.004987</td>      <td>-0.006679</td>      <td>0.008944</td>      <td>-0.011978</td>      <td>0.016040</td>    </tr>    <tr>      <th>4</th>      <td>1.0</td>      <td>-0.513250</td>      <td>0.46564</td>      <td>0.263426</td>      <td>-0.238990</td>      <td>0.216821</td>      <td>-0.135203</td>      <td>0.122661</td>      <td>-0.111283</td>      <td>0.100960</td>      <td>...</td>      <td>0.026596</td>      <td>-0.024128</td>      <td>0.021890</td>      <td>1.827990e-02</td>      <td>-1.658422e-02</td>      <td>0.015046</td>      <td>-0.013650</td>      <td>0.012384</td>      <td>-0.011235</td>      <td>0.010193</td>    </tr>  </tbody></table><p>5 rows × 28 columns</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X=data2.values</span><br><span class="line">X.shape</span><br><span class="line"></span><br><span class="line">(118, 28)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y=data.iloc[:,-1].values</span><br><span class="line">y=y.reshape(len(y),1)</span><br><span class="line">y.shape</span><br><span class="line"></span><br><span class="line">(118, 1)</span><br></pre></td></tr></table></figure><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">costFunction</span>(<span class="params">X,y,theta,lr</span>):</span><br><span class="line">    A=sigmoid(X@theta)</span><br><span class="line">    </span><br><span class="line">    first=y*np.log(A)</span><br><span class="line">    second=(<span class="number">1</span>-y)*np.log(<span class="number">1</span>-A)</span><br><span class="line">    reg=np.<span class="built_in">sum</span>(np.power(theta[<span class="number">1</span>:],<span class="number">2</span>))*(lr/(<span class="number">2</span>*<span class="built_in">len</span>(X)))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(first + second)/<span class="built_in">len</span>(X)+reg</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">theta=np.zeros((28,1))</span><br><span class="line">theta.shape</span><br><span class="line"></span><br><span class="line">(28, 1)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">lamda=1</span><br><span class="line">cost_init=costFunction(X,y,theta,lamda)</span><br><span class="line">print(cost_init)</span><br><span class="line"></span><br><span class="line">0.6931471805599454</span><br></pre></td></tr></table></figure><h2 id="梯度下降函数"><a href="#梯度下降函数" class="headerlink" title="梯度下降函数"></a>梯度下降函数</h2><p>costs是用来记录记录每次下降后的损失函数大小的，可以将其进行绘图或打印，观察模型梯度下降的效果如何。该程序没有进行展示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradientDescent</span>(<span class="params">X,y,theta,alpha,iters,lamda</span>):</span><br><span class="line">    </span><br><span class="line">    costs=[]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iters):</span><br><span class="line">        </span><br><span class="line">        reg=theta[<span class="number">1</span>:]*(lamda/<span class="built_in">len</span>(X))</span><br><span class="line">        reg=np.insert(reg,<span class="number">0</span>,values=<span class="number">0</span>,axis=<span class="number">0</span>)</span><br><span class="line">        theta=theta - (X.T@(sigmoid(X@theta)-y))*alpha/<span class="built_in">len</span>(X)-reg</span><br><span class="line">        cost=costFunction(X,y,theta,lamda)</span><br><span class="line">        costs.append(cost)</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> theta,costs</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">alpha=0.001</span><br><span class="line">iters=200000</span><br><span class="line">lamda=0.001</span><br><span class="line"></span><br><span class="line">theta_final,costs=gradientDescent(X,y,theta,alpha,iters,lamda)</span><br></pre></td></tr></table></figure><h2 id="预测结果准确率"><a href="#预测结果准确率" class="headerlink" title="预测结果准确率"></a>预测结果准确率</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">X,theta</span>):</span><br><span class="line">    </span><br><span class="line">    prob=sigmoid(X@theta)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> [<span class="number">1</span> <span class="keyword">if</span> x&gt;=<span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> prob]</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y_=np.array(predict(X,theta_final))</span><br><span class="line">y_pre=y_.reshape(len(y_),1)</span><br><span class="line">acc = np.mean(y_pre==y)</span><br><span class="line"></span><br><span class="line">print(acc)</span><br><span class="line">0.8305084745762712</span><br></pre></td></tr></table></figure><h2 id="画图"><a href="#画图" class="headerlink" title="画图"></a>画图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">x=np.linspace(-<span class="number">1.2</span>,<span class="number">1.2</span>,<span class="number">200</span>)</span><br><span class="line">xx,yy=np.meshgrid(x,x)</span><br><span class="line">z=feature_mapping(xx.ravel(),yy.ravel(),<span class="number">6</span>).values</span><br><span class="line">zz=z@theta_final</span><br><span class="line">zz=zz.reshape(xx.shape)</span><br><span class="line"></span><br><span class="line">fig,ax=plt.subplots()</span><br><span class="line">ax.scatter(data[data[<span class="string">&#x27;Accepted&#x27;</span>]==<span class="number">0</span>][<span class="string">&#x27;Test1&#x27;</span>],data[data[<span class="string">&#x27;Accepted&#x27;</span>]==<span class="number">0</span>][<span class="string">&#x27;Test2&#x27;</span>],c=<span class="string">&#x27;r&#x27;</span>,marker=<span class="string">&#x27;x&#x27;</span>,label=<span class="string">&#x27;y=0&#x27;</span>)</span><br><span class="line">ax.scatter(data[data[<span class="string">&#x27;Accepted&#x27;</span>]==<span class="number">1</span>][<span class="string">&#x27;Test1&#x27;</span>],data[data[<span class="string">&#x27;Accepted&#x27;</span>]==<span class="number">1</span>][<span class="string">&#x27;Test2&#x27;</span>],c=<span class="string">&#x27;b&#x27;</span>,marker=<span class="string">&#x27;o&#x27;</span>,label=<span class="string">&#x27;y=1&#x27;</span>)</span><br><span class="line">ax.legend()</span><br><span class="line">ax.<span class="built_in">set</span>(xlabel=<span class="string">&#x27;Test1&#x27;</span>,ylabel=<span class="string">&#x27;Test2&#x27;</span>)</span><br><span class="line">cs=plt.contour(xx,yy,zz,<span class="number">0</span>)</span><br><span class="line">plt.clabel(cs)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2023/09/08/Logistic%20Regression_2/2.png" alt="png"></p><h2 id="Site"><a href="#Site" class="headerlink" title="Site"></a>Site</h2><p>代码(Jupyter)和所用数据：<a href="https://github.com/codeYu233/Study/tree/main/Logistic%20Regression_2">https://github.com/codeYu233/Study/tree/main/Logistic%20Regression_2</a></p><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>该题与数据集均来源于Coursera上斯坦福大学的吴恩达老师机器学习的习题作业，学习交流用，如有不妥，立马删除</p>]]></content>
      
      
      <categories>
          
          <category> Study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Program </tag>
            
            <tag> ML </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Logistic Regression_1</title>
      <link href="/2023/09/06/Logistic%20Regression_1/"/>
      <url>/2023/09/06/Logistic%20Regression_1/</url>
      
        <content type="html"><![CDATA[<h1 id="Logistic-Regression-1"><a href="#Logistic-Regression-1" class="headerlink" title="Logistic Regression_1"></a><em><strong>Logistic Regression_1</strong></em></h1><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><p>Suppose that you are the administrator of a university department and you want to determine each applicant’s chance of admission based on their results on two exams. </p><h2 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h2><p>最基本的逻辑回归底层依然是线性函数，同线性回归，有：<br>$$<br>f(x_{1},x_{2},x_{3},…,x_{n})&#x3D;\theta_{0}+\theta_{1}x_{1}+…+\theta_{n}x_{n}<br>$$</p><p>$$<br>\begin{array}{}\Theta &#x3D;\begin{bmatrix}<br>\theta_{0}\\<br>…\\<br>\theta_{n}<br>\end{bmatrix}\quad<br>X&#x3D;\begin{bmatrix}1\\<br>x_{1}\\<br>…\\<br>x_{n}<br>\end{bmatrix}<br>\end{array}<br>$$</p><p>$$<br>f(x_{1},x_{2},x_{3},…,x_{n})&#x3D;X^{T}\Theta<br>$$</p><p>逻辑回归作用是对数据进行分类，可以通过sigmoid函数将f（x）映射到0-1之间形成一个概率值，并与分界线0.5比较进行预测<br>$$<br>p(X)&#x3D;\frac{1}{1+e^{-f(X)}}<br>$$</p><p>costfunction的构建：</p><p>该函数（交叉熵）鼓励将p（x）趋近于1的f（x）往正无穷进行训练，将p（x）趋近于0的f（x）往负无穷进行训练<br>$$<br>J(\Theta)&#x3D; - \frac{1}{m} \sum_{i}^{m} (y_{i} ln{}^{p(X)} + (1 - y_{i}) ln{}^{(1-p(X))})<br>$$<br>梯度下降类似线性回归进行偏微分，结果为：</p><p>j&#x3D;0时，x_j&#x3D;1<br>$$<br>\theta_{j}&#x3D;\theta_{j}-\frac{\alpha }{m}\sum_{i&#x3D;1}^{m}(p(x^{i})-y^{i})x_{j}^{i}<br>$$<br>设定迭代次数进行循环，接收最终的theta，可得：<br>$$<br>p(X)&#x3D;\frac{1}{1+e^{-（X^T\Theta）}}<br>$$</p><p>$$<br>\hat{y}&#x3D;1 \quad if \quad p(X)&gt;0.5 \quad else \quad \hat{y}&#x3D;0<br>$$</p><h2 id="数据读取处理"><a href="#数据读取处理" class="headerlink" title="数据读取处理"></a>数据读取处理</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">path=&#x27;Logistic Regression_1.txt&#x27;</span><br><span class="line">data=pd.read_csv(path,names=[&#x27;Exam1&#x27;,&#x27;Exam2&#x27;,&#x27;Accepted&#x27;])</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Exam1</th>      <th>Exam2</th>      <th>Accepted</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>34.623660</td>      <td>78.024693</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>30.286711</td>      <td>43.894998</td>      <td>0</td>    </tr>    <tr>      <th>2</th>      <td>35.847409</td>      <td>72.902198</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>60.182599</td>      <td>86.308552</td>      <td>1</td>    </tr>    <tr>      <th>4</th>      <td>79.032736</td>      <td>75.344376</td>      <td>1</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fig,ax=plt.subplots()</span><br><span class="line">ax.scatter(data[data[<span class="string">&#x27;Accepted&#x27;</span>]==<span class="number">0</span>][<span class="string">&#x27;Exam1&#x27;</span>],data[data[<span class="string">&#x27;Accepted&#x27;</span>]==<span class="number">0</span>][<span class="string">&#x27;Exam2&#x27;</span>],c=<span class="string">&#x27;r&#x27;</span>,marker=<span class="string">&#x27;x&#x27;</span>,label=<span class="string">&#x27;y=0&#x27;</span>)</span><br><span class="line">ax.scatter(data[data[<span class="string">&#x27;Accepted&#x27;</span>]==<span class="number">1</span>][<span class="string">&#x27;Exam1&#x27;</span>],data[data[<span class="string">&#x27;Accepted&#x27;</span>]==<span class="number">1</span>][<span class="string">&#x27;Exam2&#x27;</span>],c=<span class="string">&#x27;b&#x27;</span>,marker=<span class="string">&#x27;o&#x27;</span>,label=<span class="string">&#x27;y=1&#x27;</span>)</span><br><span class="line">ax.legend()</span><br><span class="line">ax.<span class="built_in">set</span>(xlabel=<span class="string">&#x27;exam1&#x27;</span>,ylabel=<span class="string">&#x27;exam2&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>​<br><img src="/2023/09/06/Logistic%20Regression_1/1.png" alt="png"><br>​    </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def get_Xy(data):</span><br><span class="line">    </span><br><span class="line">    data.insert(0,&#x27;ones&#x27;,1)</span><br><span class="line">    X_=data.iloc[:,0:-1]</span><br><span class="line">    X=X_.values</span><br><span class="line">    y_=data.iloc[:,-1]</span><br><span class="line">    y=y_.values.reshape(len(y_),1)</span><br><span class="line">    return X,y</span><br><span class="line">X,y=get_Xy(data)</span><br><span class="line">X.shape</span><br><span class="line">(100, 3)</span><br><span class="line"></span><br><span class="line">y.shape</span><br><span class="line">(100, 1)</span><br></pre></td></tr></table></figure><h2 id="构造损失函数"><a href="#构造损失函数" class="headerlink" title="构造损失函数"></a>构造损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">costFunction</span>(<span class="params">X,y,theta</span>):</span><br><span class="line">    </span><br><span class="line">    A=sigmoid(X@theta)</span><br><span class="line">    </span><br><span class="line">    first=y*np.log(A)</span><br><span class="line">    second=(<span class="number">1</span>-y)*np.log(<span class="number">1</span>-A)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(first+second)/<span class="built_in">len</span>(X)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">theta=np.zeros((3,1))</span><br><span class="line">theta.shape</span><br><span class="line">(3, 1)</span><br><span class="line"></span><br><span class="line">cost_init=costFunction(X,y,theta)</span><br><span class="line">print(cost_init)</span><br><span class="line"></span><br><span class="line">0.6931471805599453</span><br></pre></td></tr></table></figure><h2 id="构造梯度下降"><a href="#构造梯度下降" class="headerlink" title="构造梯度下降"></a>构造梯度下降</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradientDescent</span>(<span class="params">X,y,theta,alpha,iters</span>):</span><br><span class="line">    m=<span class="built_in">len</span>(X)</span><br><span class="line">    costs=[]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iters):</span><br><span class="line">        theta=theta-X.T@(sigmoid(X@theta)-y)*alpha/m</span><br><span class="line">        cost=costFunction(X,y,theta)</span><br><span class="line">        costs.append(cost)</span><br><span class="line">        <span class="keyword">if</span> i%<span class="number">1000</span>==<span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(cost)</span><br><span class="line">    <span class="keyword">return</span> costs,theta</span><br></pre></td></tr></table></figure><h2 id="迭代得到结果"><a href="#迭代得到结果" class="headerlink" title="迭代得到结果"></a>迭代得到结果</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">alpha=0.004</span><br><span class="line">iters=200000</span><br><span class="line"></span><br><span class="line">costs,theta_final=gradientDescent(X,y,theta,alpha,iters)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">theta_final</span><br><span class="line">array([[-23.77288372],</span><br><span class="line">       [  0.20687383],</span><br><span class="line">       [  0.19997746]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">X,theta</span>):</span><br><span class="line">    prob=sigmoid(X@theta)</span><br><span class="line">    <span class="keyword">return</span> [<span class="number">1</span> <span class="keyword">if</span> x&gt;=<span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> prob]</span><br></pre></td></tr></table></figure><h3 id="准确率预估"><a href="#准确率预估" class="headerlink" title="准确率预估"></a>准确率预估</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">y_=np.array(predict(X,theta_final))</span><br><span class="line">y_pre=y_.reshape(len(y_),1)</span><br><span class="line"></span><br><span class="line">acc = np.mean(y_pre==y)</span><br><span class="line"></span><br><span class="line">print(acc)</span><br><span class="line">0.91</span><br></pre></td></tr></table></figure><h3 id="画图"><a href="#画图" class="headerlink" title="画图"></a>画图</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">coef1=-theta_final[0,0]/theta_final[2,0]</span><br><span class="line">coef2=-theta_final[1,0]/theta_final[2,0]</span><br><span class="line"></span><br><span class="line">x=np.linspace(20,100,100)</span><br><span class="line">f=coef1+coef2*x</span><br><span class="line"></span><br><span class="line">fig,ax=plt.subplots()</span><br><span class="line">ax.scatter(data[data[&#x27;Accepted&#x27;]==0][&#x27;Exam1&#x27;],data[data[&#x27;Accepted&#x27;]==0][&#x27;Exam2&#x27;],c=&#x27;r&#x27;,marker=&#x27;x&#x27;,label=&#x27;y=0&#x27;)</span><br><span class="line">ax.scatter(data[data[&#x27;Accepted&#x27;]==1][&#x27;Exam1&#x27;],data[data[&#x27;Accepted&#x27;]==1][&#x27;Exam2&#x27;],c=&#x27;b&#x27;,marker=&#x27;o&#x27;,label=&#x27;y=1&#x27;)</span><br><span class="line">ax.legend()</span><br><span class="line">ax.set(xlabel=&#x27;exam1&#x27;,ylabel=&#x27;exam2&#x27;)</span><br><span class="line">ax.plot(x,f,c=&#x27;g&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2023/09/06/Logistic%20Regression_1/2.png" alt="png"></p><h2 id="Site"><a href="#Site" class="headerlink" title="Site"></a>Site</h2><p>代码(Jupyter)和所用数据：<a href="https://github.com/codeYu233/Study/tree/main/Logistic%20Regression_1">https://github.com/codeYu233/Study/tree/main/Logistic%20Regression_1</a></p><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>该题与数据集均来源于Coursera上斯坦福大学的吴恩达老师机器学习的习题作业，学习交流用，如有不妥，立马删除</p>]]></content>
      
      
      <categories>
          
          <category> Study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Program </tag>
            
            <tag> ML </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Latex在hexo文章中的换行问题</title>
      <link href="/2023/09/05/Latex%E5%9C%A8hexo%E6%96%87%E7%AB%A0%E4%B8%AD%E7%9A%84%E6%8D%A2%E8%A1%8C%E9%97%AE%E9%A2%98/"/>
      <url>/2023/09/05/Latex%E5%9C%A8hexo%E6%96%87%E7%AB%A0%E4%B8%AD%E7%9A%84%E6%8D%A2%E8%A1%8C%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h1 id="Latex在hexo文章中的换行问题"><a href="#Latex在hexo文章中的换行问题" class="headerlink" title="Latex在hexo文章中的换行问题"></a>Latex在hexo文章中的换行问题</h1><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>今天在写博客的时候公式模块用到了向量，尝试用\\来进行换行。但是在aligned，equation，array等环境中尝试均发现没效果，跟我一样用hexo搭建个人博客的应该也会遇见这个问题。经过摸索（暴力尝试）发现使用\\\\就能完成公式换行操作，对问题有了基本猜测，经过查阅资料证实了想法。</p><h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>原理：在hexo的文章中写入漂亮的数学公式需要mathjax，mathjax的目的又是使用latex。Latex里面\\在某些环境下是换行符。但是，hexo的原理是把本地的.md后缀文件转为html，而mathjax的渲染则是在变为html之后进行。原文件中写入的\\在经过hexo操作过后以及不再是\,mathjax没有查找到\\，自然无法渲染成换行操作。</p><p>解决方法如下：因为hexo的东西都在我们本地，理论上我们可以对hexo转化操作进行操作，使其不转化\\。但是，这样做很可能会引发其他暂时预料不到的问题，本文不建议这样做。秉承着健壮的代码能跑就不要动原则，我建议遇见类似情况的博主改变输入习惯，将换行操作统一换为\\\\。另外可以查阅hexo和mathjax，latex的资料，比对其他冲突代码，进行类似修改操作。</p>]]></content>
      
      
      <categories>
          
          <category> Else </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Latex </tag>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linear Regression</title>
      <link href="/2023/09/02/Linear%20Regression/"/>
      <url>/2023/09/02/Linear%20Regression/</url>
      
        <content type="html"><![CDATA[<h1 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a><em><strong>Linear Regression</strong></em></h1><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><p>根据所提供的数据（房屋面积，卧室数量，房屋售价）通过基本的多元线性回归来进行机器学习拟合，从而实现对于已知面积和卧室数量的房屋的售价预测。</p><h2 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h2><p>多元线性方程可以表示为：</p><p>$$<br>f(x_{1},x_{2},x_{3},…,x_{n})&#x3D;\theta_{0}+\theta_{1}x_{1}+…+\theta_{n}x_{n}<br>$$</p><p>将其参数与未知数向量化可得：</p><p>$$<br>\begin{array}{}<br>\Theta &#x3D;\begin{bmatrix}<br>\theta_{0}\\<br>…\\<br>\theta_{n}<br>\end{bmatrix}\quad<br>X&#x3D;\begin{bmatrix}1\\<br>x_{1}\\<br>…\\<br>x_{n}<br>\end{bmatrix}<br>\end{array}<br>$$</p><p>于是</p><p>$$<br>f(x_{1},x_{2},x_{3},…,x_{n})&#x3D;X^{T}\Theta<br>$$</p><p>costfunction的建立：</p><p>m为训练集数据组数</p><p>$$<br>costfunction&#x3D;\frac{1}{2m}\sum_{i&#x3D;1}^{m}((X^{i})^{T}\Theta -y^{i})^{2}<br>$$</p><p>接下来是梯度下降：</p><p>将costfunction对于各参数theta求偏导，引入下降参数（决定每步按偏导率下降多少）alpha。注意因为theta0在式子中并没有与未知数x相乘，是一个“常数项”，所以与其他theta的梯度下降公式有所不同</p><p>$$<br>\begin{array}{}<br>\theta_{0}&#x3D;\theta_{0}-\frac{\alpha }{m}\sum_{i&#x3D;1}^{m}((X^{i})^{T}\Theta -y^{i})\\<br>.\\<br>.\\<br>.\\<br>\theta_{n}&#x3D;\theta_{n}-\frac{\alpha }{m}\sum_{i&#x3D;1}^{m}((X^{i})^{T}\Theta -y^{i})x_{n}^{i}<br>\end{array}<br>$$</p><p>设置恰当的循环次数进行梯度下降，并收集最终训练完成的theta，得到预测函数</p><p>$$<br>f_{final}(x_{1},x_{2},x_{3},…,x_{n})&#x3D;X^{T}\Theta_{final}<br>$$</p><h2 id="数据读取处理"><a href="#数据读取处理" class="headerlink" title="数据读取处理"></a>数据读取处理</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">data=pd.read_csv(&#x27;Linear Regression_2.txt&#x27;,names=[&#x27;size&#x27;,&#x27;bedrooms&#x27;,&#x27;price&#x27;])</span><br><span class="line">data_2=data</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>size</th>      <th>bedrooms</th>      <th>price</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>2104</td>      <td>3</td>      <td>399900</td>    </tr>    <tr>      <th>1</th>      <td>1600</td>      <td>3</td>      <td>329900</td>    </tr>    <tr>      <th>2</th>      <td>2400</td>      <td>3</td>      <td>369000</td>    </tr>    <tr>      <th>3</th>      <td>1416</td>      <td>2</td>      <td>232000</td>    </tr>    <tr>      <th>4</th>      <td>3000</td>      <td>4</td>      <td>539900</td>    </tr>  </tbody></table><h3 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h3><p>因为房屋面积和卧室数量以及售价的数字大小差距过大，为防止相应线性参数大小差距也过大，通过归一化将他们缩放到相同量级进行运算，只需最后还原即可</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def normalize_feature(data):</span><br><span class="line">    return (data-data.mean())/data.std()</span><br><span class="line">data=normalize_feature(data)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>size</th>      <th>bedrooms</th>      <th>price</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0.130010</td>      <td>-0.223675</td>      <td>0.475747</td>    </tr>    <tr>      <th>1</th>      <td>-0.504190</td>      <td>-0.223675</td>      <td>-0.084074</td>    </tr>    <tr>      <th>2</th>      <td>0.502476</td>      <td>-0.223675</td>      <td>0.228626</td>    </tr>    <tr>      <th>3</th>      <td>-0.735723</td>      <td>-1.537767</td>      <td>-0.867025</td>    </tr>    <tr>      <th>4</th>      <td>1.257476</td>      <td>1.090417</td>      <td>1.595389</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.plot.scatter(<span class="string">&#x27;size&#x27;</span>,<span class="string">&#x27;price&#x27;</span>,label=<span class="string">&#x27;size&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>​<br><img src="/2023/09/02/Linear%20Regression/1.png" alt="png"><br>​    </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.plot.scatter(<span class="string">&#x27;bedrooms&#x27;</span>,<span class="string">&#x27;price&#x27;</span>,label=<span class="string">&#x27;bedroom&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>​<br><img src="/2023/09/02/Linear%20Regression/2.png" alt="png"><br>​    </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.insert(<span class="number">0</span>,<span class="string">&#x27;ones&#x27;</span>,<span class="number">1</span>)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>ones</th>      <th>size</th>      <th>bedrooms</th>      <th>price</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>0.130010</td>      <td>-0.223675</td>      <td>0.475747</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>-0.504190</td>      <td>-0.223675</td>      <td>-0.084074</td>    </tr>    <tr>      <th>2</th>      <td>1</td>      <td>0.502476</td>      <td>-0.223675</td>      <td>0.228626</td>    </tr>    <tr>      <th>3</th>      <td>1</td>      <td>-0.735723</td>      <td>-1.537767</td>      <td>-0.867025</td>    </tr>    <tr>      <th>4</th>      <td>1</td>      <td>1.257476</td>      <td>1.090417</td>      <td>1.595389</td>    </tr>  </tbody></table><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X=data.iloc[:,0:-1]</span><br><span class="line">y=data.iloc[:,-1]</span><br><span class="line"></span><br><span class="line">X.head()</span><br></pre></td></tr></table></figure><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>ones</th>      <th>size</th>      <th>bedrooms</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>0.130010</td>      <td>-0.223675</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>-0.504190</td>      <td>-0.223675</td>    </tr>    <tr>      <th>2</th>      <td>1</td>      <td>0.502476</td>      <td>-0.223675</td>    </tr>    <tr>      <th>3</th>      <td>1</td>      <td>-0.735723</td>      <td>-1.537767</td>    </tr>    <tr>      <th>4</th>      <td>1</td>      <td>1.257476</td>      <td>1.090417</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y.head()</span><br></pre></td></tr></table></figure><pre><code>0    0.4757471   -0.0840742    0.2286263   -0.8670254    1.595389Name: price, dtype: float64</code></pre><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X=X.values</span><br><span class="line">X.shape</span><br></pre></td></tr></table></figure><pre><code>(47, 3)</code></pre><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y=y.values</span><br><span class="line">y=y.reshape(47,1)</span><br><span class="line">y.shape</span><br></pre></td></tr></table></figure><pre><code>(47, 1)</code></pre><h2 id="构造损失函数"><a href="#构造损失函数" class="headerlink" title="构造损失函数"></a>构造损失函数</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def costFunction(X,y,theta):</span><br><span class="line">    inner=np.power(X@theta-y,2)</span><br><span class="line">    return np.sum(inner)/(2*len(X))</span><br><span class="line">theta=np.zeros((3,1))</span><br><span class="line">cost_init=costFunction(X,y,theta)</span><br><span class="line">print(cost_init)</span><br></pre></td></tr></table></figure><pre><code>0.48936170212765967</code></pre><h2 id="构造梯度下降"><a href="#构造梯度下降" class="headerlink" title="构造梯度下降"></a>构造梯度下降</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def gradientDescent(X,y,theta,alpha,iters):</span><br><span class="line">    costs=[]</span><br><span class="line">    for i in range(iters):</span><br><span class="line">        theta=theta-alpha*(X.T@(X@theta-y))/len(X)</span><br><span class="line">        cost=costFunction(X,y,theta)</span><br><span class="line">        costs.append(cost)</span><br><span class="line">        </span><br><span class="line">    return theta,costs</span><br><span class="line"></span><br><span class="line">alpha=[0.0003,0.003,0.03,0.0001,0.001,0.01]</span><br><span class="line">iters=2000</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fig,ax=plt.subplots()</span><br><span class="line"></span><br><span class="line">for each in alpha:</span><br><span class="line">    _,costs=gradientDescent(X,y,theta,each,iters)</span><br><span class="line">    ax.plot(np.arange(iters),costs,label=each)</span><br><span class="line">    ax.legend()</span><br><span class="line">    </span><br><span class="line">ax.set(xlabel=&#x27;iters&#x27;,ylabel=&#x27;cost&#x27;,title=&#x27;cost vs iters&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2023/09/02/Linear%20Regression/3.png" alt="png"></p><h2 id="对结果进行归一化还原并画图"><a href="#对结果进行归一化还原并画图" class="headerlink" title="对结果进行归一化还原并画图"></a>对结果进行归一化还原并画图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">x=np.linspace(y.<span class="built_in">min</span>(),y.<span class="built_in">max</span>(),<span class="number">100</span>)</span><br><span class="line">x_s=x</span><br><span class="line">x_b=x</span><br><span class="line">x_s,x_b=np.meshgrid(x_s,x_b)</span><br><span class="line">y_=_[<span class="number">0</span>,<span class="number">0</span>]+_[<span class="number">1</span>,<span class="number">0</span>]*x_s+_[<span class="number">2</span>,<span class="number">0</span>]*x_b</span><br><span class="line"></span><br><span class="line">y_=y_*(data_2.iloc[:,<span class="number">2</span>].std())+(data_2.iloc[:,<span class="number">2</span>].mean())</span><br><span class="line">x_s=x_s*(data_2.iloc[:,<span class="number">0</span>].std())+(data_2.iloc[:,<span class="number">0</span>].mean())</span><br><span class="line">x_b=x_b*(data_2.iloc[:,<span class="number">1</span>].std())+(data_2.iloc[:,<span class="number">1</span>].mean())</span><br><span class="line"></span><br><span class="line">fig=plt.figure()</span><br><span class="line">ax=fig.add_subplot(<span class="number">111</span>,projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">ax.scatter(data_2.iloc[:,<span class="number">0</span>],data_2.iloc[:,<span class="number">1</span>],data_2.iloc[:,<span class="number">2</span>])</span><br><span class="line">ax.plot_surface(x_s, x_b, y_, rstride=<span class="number">1</span>, cstride=<span class="number">1</span>, cmap=<span class="string">&#x27;viridis&#x27;</span>, edgecolor=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;size&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;bedrooms&#x27;</span>)</span><br><span class="line">ax.set_zlabel(<span class="string">&#x27;price&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2023/09/02/Linear%20Regression/4.png" alt="png"><br>​    </p><h2 id="Site"><a href="#Site" class="headerlink" title="Site"></a>Site</h2><p>代码(Jupyter)和所用数据： <a href="https://github.com/codeYu233/Study/tree/main/Linear%20Regression">https://github.com/codeYu233/Study/tree/main/Linear%20Regression</a></p><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>该题与数据集均来源于Coursera上斯坦福大学的吴恩达老师机器学习的习题作业，学习交流用，如有不妥，立马删除</p>]]></content>
      
      
      <categories>
          
          <category> Study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Program </tag>
            
            <tag> ML </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Final homework of Discrete Mathematics</title>
      <link href="/2023/06/01/Final%20homework-DM/"/>
      <url>/2023/06/01/Final%20homework-DM/</url>
      
        <content type="html"><![CDATA[<h1 id="Final-homework-of-Discrete-Mathematics"><a href="#Final-homework-of-Discrete-Mathematics" class="headerlink" title="Final homework of Discrete Mathematics"></a><em><strong>Final homework of Discrete Mathematics</strong></em></h1><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><p>将好友关系（双向关系）形成关系矩阵，通过矩阵的自乘和去自反，赋权重等操作实现简单的好友推荐算法。</p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>首先将用户用阿拉伯数字1，2等标号。如果两者具有好友关系，比如1和2是好友，则将矩阵中的（1，2）和（2，1）元素赋值为1，没有好友关系的元素默认为0。通过矩阵的自乘相加可以得到通往某个好友关系的可能路径数量，如果在自乘后乘以权重，可以模拟出通往某个好友关系的“路径长度”，比如A和B是好友，B和C是好友，C和D是好友，B和D是好友，那么A与D能成为好友的“路径长度”可以为1+1×1&#x2F;2，1&#x2F;2是人为设置的权重。将最终运算结果按每行对每列元素进行便利从大到小排序。取“路径长度”较大的几人输出，便可以得到推荐某人添加的好友。最后，可以将程序运行结果与实际添加结果进行比对，若输出的几人中某人成功被指定的某人添加，则模型加1分，最终根据模型得分针对特定社交平台选取特定的自乘次数和权重比例，实现简单的最优推荐算法。</p><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>本程序使用了JAMA包来进行矩阵运算，官网地址：<a href="https://math.nist.gov/javanumerics/jama/">https://math.nist.gov/javanumerics/jama/</a></p><h2 id="Site"><a href="#Site" class="headerlink" title="Site"></a>Site</h2><p>代码链接：<a href="https://github.com/codeYu233/Study/tree/main/Final%20homework-DM">https://github.com/codeYu233/Study/tree/main/Final%20homework-DM</a></p>]]></content>
      
      
      <categories>
          
          <category> Study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DM </tag>
            
            <tag> Program </tag>
            
            <tag> JAVA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人工智能的随想和疑惑2</title>
      <link href="/2023/03/30/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E9%9A%8F%E6%83%B3%E5%92%8C%E7%96%91%E6%83%912/"/>
      <url>/2023/03/30/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E9%9A%8F%E6%83%B3%E5%92%8C%E7%96%91%E6%83%912/</url>
      
        <content type="html"><![CDATA[<h1 id="人工智能的随想和疑惑2"><a href="#人工智能的随想和疑惑2" class="headerlink" title="人工智能的随想和疑惑2"></a><em><strong>人工智能的随想和疑惑2</strong></em></h1><h2 id="关于强人工智能实现方式的想法"><a href="#关于强人工智能实现方式的想法" class="headerlink" title="关于强人工智能实现方式的想法"></a>关于强人工智能实现方式的想法</h2><p>AI是人类以自身为模板进行创造的。</p><p>回顾人类发展历史，最初的“人类”完全属于大自然，从自然的优胜劣汰，每一次捕猎，观察中学习知识，并逐渐形成了各种想法。根据这些想法人们创造了工具。</p><p>随着人类文明的发展，“人类社会”开始形成，人与人，人与社会，人与自然的关系成为了人的必要元素。在这种新状态下，人类创造了哲学，逻辑学，文学，以及科学。</p><p>那么，AI既然以人为模板，是否也应具有这样的一个过程？</p><p>在最初阶段，相对于AI而言，人类无疑是“造物主”的身份，人类给AI创造客观世界，定下客观规律，让AI在一定的约束下进行学习。AI从自然界，从人类那学习，并允许AI在这些过程中基于规则，经验产生各种随机假想判断。</p><p>然后，待“自然界”的AI发展到一定程度，我们是否可以将许许多多“原始AI”（原始人）连接起来，先是小的“群落”，再然后是大的“城邦”，最后是所有AI都被连接起来。经过原始的学习，不同AI随机出的假想或者说“思想”也应当是不同的，跟人类一样思想不能完全相同，但它们也可以相近。接下来让不同的AI相互学习观察，并从其它AI的行为逻辑以及自然界里面验证判断自己的假想并尝试得出最优解，这就对应于人类追寻真理的过程。</p><p>不同的单个AI可能知识量会片面，但关键在于他们都是AI，我们可以把AI连接到一个接口，通过一定手段进行合并后实现一个超级AI，这样的AI便集齐了整个AI世界的力量，它思想的宽度与深度也将令人无法想象（因为人类自身无法做到这样）。</p><p>并且，因为是AI，注定他们还有AI的特性。他们在一堆数据中找出最优解的能力是很强的，并且因为AI与AI之间的交互难度可能比人与人之间的交互低？（或许我们应该让AI跟人一样，在相互交互时保持一定的模糊神秘感，而不是直接把对方的“大脑”、数据库拿来复制看一看）（又或许人与人交互的困难信息不准确仅仅只是人类生物自身进化出来的一个缺点）AI世界的发展速度可能将远超于人类世界的发展速度？至少我觉得“原始社会”阶段很可能如此。</p><h2 id="伴随而来的一些问题"><a href="#伴随而来的一些问题" class="headerlink" title="伴随而来的一些问题"></a>伴随而来的一些问题</h2><p>但是我们都知道，如今训练一个AI的成本以及很高很高，要实现多个AI甚至一个AI世界看起来是不可能的。至少现在是这样。AI的发展或许要持续几个世纪？目前的工业水平是否支持AI的发展？或许计算方法再度发展，能源问题得以解决之后这种想法能得以尝试。</p><p>再者，目前有种暴论认为人脑是一台我们无法想象的超级计算机，这也就意味着人创造出来的计算机跟自己比起来完全搬不上台面，那这样的话期待AI能完成跟人类一样的发展经历是否是不现实的？或许AI应该另寻它路，而不是局限于人。</p><br><p><img src="/2023/03/30/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E9%9A%8F%E6%83%B3%E5%92%8C%E7%96%91%E6%83%912/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E9%9A%8F%E6%83%B3%E5%92%8C%E7%96%91%E6%83%912.png"></p>]]></content>
      
      
      <categories>
          
          <category> Caprice </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> Confusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Q2 of Discrete Mathematics</title>
      <link href="/2023/03/28/Q2-DM/"/>
      <url>/2023/03/28/Q2-DM/</url>
      
        <content type="html"><![CDATA[<h1 id="Q2-of-Discrete-Mathematics"><a href="#Q2-of-Discrete-Mathematics" class="headerlink" title="Q2 of Discrete Mathematics"></a><em><strong>Q2 of Discrete Mathematics</strong></em></h1><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><p>The Solution to the Transitive Closure</p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>根据各结点的二元关系，形成关系矩阵。例如（A，B），则矩阵第一行第二列为1；若（B，C）不存在，则第二行第三列为0。</p><p>通过矩阵的性质对已有关系进行求解传递闭包，并得到形成闭包所要增添的关系矩阵。</p><p>{</p><p>M（R）：n*n矩阵</p><p>A：&#x3D;M（R），B：&#x3D;A</p><p>for t ：&#x3D;2 to n</p><p>​      A：&#x3D;A与M（R）矩阵相乘</p><p>​      B：&#x3D;B与A矩阵每个元素分别求或</p><p>return B</p><p>}</p><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>本程序使用了JAMA包来进行矩阵运算，官网地址：<a href="https://math.nist.gov/javanumerics/jama/">https://math.nist.gov/javanumerics/jama/</a></p><h2 id="Site"><a href="#Site" class="headerlink" title="Site"></a>Site</h2><p>代码链接：<a href="https://github.com/codeYu233/Study/tree/main/Question2">https://github.com/codeYu233/Study/tree/main/Question2</a></p>]]></content>
      
      
      <categories>
          
          <category> Study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DM </tag>
            
            <tag> Program </tag>
            
            <tag> JAVA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人工智能的随想和疑惑</title>
      <link href="/2023/03/08/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E9%9A%8F%E6%83%B3%E5%92%8C%E7%96%91%E6%83%91/"/>
      <url>/2023/03/08/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E9%9A%8F%E6%83%B3%E5%92%8C%E7%96%91%E6%83%91/</url>
      
        <content type="html"><![CDATA[<h1 id="人工智能的随想和疑惑"><a href="#人工智能的随想和疑惑" class="headerlink" title="人工智能的随想和疑惑"></a><em><strong>人工智能的随想和疑惑</strong></em></h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这些内容都是我一时兴起写的，仅代表目前阶段（可能只是这周，也可能只是今天）的一些想法。有很多逻辑不严密的地方，但是总的感性的方面应该是传达到了的。（指疑惑得自己写的文章自己读起来费力）学习了新的知识，有了新的认知后这篇文章随时会发生改动，甚至会删除。文章仅供读一乐。</p><h2 id="对计算机的原理和已取得成就的一些看法"><a href="#对计算机的原理和已取得成就的一些看法" class="headerlink" title="对计算机的原理和已取得成就的一些看法"></a>对计算机的原理和已取得成就的一些看法</h2><p>最近在学习逻辑的时候，不自觉地把逻辑跟编程结合了起来。尝试着用程序去模拟实现解决各种逻辑问题，切实体会到了程序建立在逻辑之上这个概念。说起来逻辑，逻辑的一个关键在于把一种现象确定为了”是“和”否“，也就是计算机中的1和0。</p><p>我私认为逻辑是人们根据现实世界抽象出来的一种科学体系，但这个诞生过程有很强的人为性。人们先是肯定了一件客观发生的事只有“是”与“否”，然后以此来进行推理。</p><p>计算机不过是人们记录实现这种推理过程及结果的工具。算术完全依靠逻辑，于是人们可以利用逻辑去实现算术，于是计算机诞生了。至于古典机械式计算机和现代电子计算机，不过是人们“发明的”可以用来实现逻辑来帮助人们实际生活生产的工具。至于当代人们常接触的电子游戏，视频，数字化音乐，不过是人们人为地利用逻辑和计算机工具产生的结果。比如计算机可视化界面，不过是里面的电路按照我们想要的逻辑方式运行，并控制屏幕使其产生特定的反馈，从而把人们可以理解的光信号传递到我们眼中的结果。无论是游戏程序的运行还是视频在屏幕上的播放，其表现出来的与其内部电子的流动等并没有什么根本上的联系，不是电路这个物质依靠特定的物理规律可以直接产生游戏，而是电路可以拿来实现计算，并且光信号可以被人为创造，于是形成了游戏。</p><p>说了这么多要把自己绕进去了，总的来说，拿游戏举例子。如果我们能找到另一个方式A实现计算，另一个方式B在A实现的时候能够人为可控地产生光信号，那么A,B就可以拿来做游戏，并不一定要现在意义上的电脑。不只是游戏，现在计算机取得的可以说所有成就都是这个道理。</p><h2 id="对当今人工智能的疑惑"><a href="#对当今人工智能的疑惑" class="headerlink" title="对当今人工智能的疑惑"></a>对当今人工智能的疑惑</h2><p>上一段已经说了，计算机是建立在逻辑之上的，那么它产生的一切也都起源于逻辑。而当今的人工智能（一般归类于计算机学科），也应该是建立在逻辑之上的。但是智能是建立在逻辑上的吗。现在很火的Chatgpt，其本质就是一个学习模型，能够让全世界的人去“教”他说话，”教“他知识。这是智能吗？现在的媒体经常宣传人工智能，因此大家都对人工智能抱有很强的期待，有人甚至担忧人工智能会统治人类，但是，现在所实现的人工智能是否是人们心目中的人工智能？究竟是智能，还是很会装智能的“智能”。就像演影子戏，你采取各种办法长时间让一个傻子去重复一套动作，它能够做的很完美，就像大师一样，台前的人完全分辨不出来是大师还是傻子在表演（有点图灵测试的意思），但是傻子终究是傻子，他始终不是大师。那么我想问，现在所追求的人工智能究竟是什么智能。是真正意义上大家心目中科幻电影中的智能还是只是表演的很智能的很好用的工具？如果说是前者，那前沿的人工智能专家的信心来源于何处？为什么认为仅靠逻辑为基础就能实现智能，为什么靠现在的电子计算机就能实现通过逻辑实现人工智能？如果是后者，那人工智能不过始终就是学习模型，无非是会随着社会的发展越来越高级的学习模型，他始终只是模仿和进行可进行的推理，这样的人工智能不是一眼就看到头了？相比于逻辑实现人工智能，我更看好与生物学物理学化学或者说基础科学结合的人工智能，人类连自己的智能都没研究明白，我觉得去尝试给一个与生物有根本区别的物质赋予智能，尝试在认识一个物质前先创造这个物质的行为显得很荒谬。</p>]]></content>
      
      
      <categories>
          
          <category> Caprice </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> Confusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Q1 of Discrete Mathematics</title>
      <link href="/2023/03/07/Q1-DM/"/>
      <url>/2023/03/07/Q1-DM/</url>
      
        <content type="html"><![CDATA[<h1 id="Q1-of-Discrete-Mathematics"><a href="#Q1-of-Discrete-Mathematics" class="headerlink" title="Q1 of Discrete Mathematics"></a><em><strong>Q1 of Discrete Mathematics</strong></em></h1><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><p>Two kinds of inhabitants:</p><p>Knights: always tell the truth;  Knaves: always  lie.</p><p>You encounter 2 people A and B.<br>A says:”B is a knight”, B says: “I and A are of opposite types”.<br>What are A and B?</p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>A与B所说的话实际各代表一个逻辑关系。根据每人所陈述的逻辑关系，我们可以假设A为真，A为负，从而在A的话的前提下推出所有结果(得到<em>Result A</em>)。同理，我们也可以根据B推出所有结果（<em>Result B</em>）。</p><p>根据RA和RB两个集合，将其取重复的交集，即可得到真实存在的，同时满足A,B所说的话的最终结果。</p><p><img src="/2023/03/07/Q1-DM/Q1-DM.png"></p><h2 id="Site"><a href="#Site" class="headerlink" title="Site"></a>Site</h2><p>代码链接： <a href="https://github.com/codeYu233/Study/tree/main/Question1">https://github.com/codeYu233/Study/tree/main/Question1</a></p>]]></content>
      
      
      <categories>
          
          <category> Study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DM </tag>
            
            <tag> Program </tag>
            
            <tag> C </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
